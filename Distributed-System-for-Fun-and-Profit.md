# Distributed Systems for Fun and Profit


* [1 [分布式系统高层视角](http://book.mixu.net/distsys/intro.html)](#1-[分布式系统高层视角](http://bookmixunet/distsys/introhtml))
	* [1.1 水平扩展 vs 垂直扩展](#11-水平扩展-vs-垂直扩展)
	* [1.2 分布式系统追求什么](#12-分布式系统追求什么)
	* [1.3 上述目标为何难以实现](#13-上述目标为何难以实现)
	* [1.4 抽象 & 建模](#14-抽象-&-建模)
	* [1.5 设计技巧：“分区”和“复制”](#15-设计技巧：“分区”和“复制”)
* [2 [关注不同抽象层次](http://book.mixu.net/distsys/abstractions.html)](#2-[关注不同抽象层次](http://bookmixunet/distsys/abstractionshtml))
	* [2.1 System Model（系统模型）](#21-system-model（系统模型）)
	* [2.2 系统模型中的节点](#22-系统模型中的节点)
	* [2.3 系统模型中的通信链路](#23-系统模型中的通信链路)
* [2.4 时间/顺序假设](#24-时间/顺序假设)
	* [2.5 共识问题（Consensus Problem）](#25-共识问题（consensus-problem）)
	* [2.6 Two impossibility results](#26-two-impossibility-results)
		* [2.6.1 The FLP impossibility result](#261-the-flp-impossibility-result)
		* [2.6.2 The CAP Theroem](#262-the-cap-theroem)
	* [2.7 一致性模型（consistency model）](#27-一致性模型（consistency-model）)
		* [2.7.1 强一致性模型 VS. 其它一致性模型](#271-强一致性模型-vs-其它一致性模型)
		* [2.7.2 强一致性模型](#272-强一致性模型)
		* [2.7.3 以client为中心的一致性模型](#273-以client为中心的一致性模型)
		* [2.7.4 最终一致性模型](#274-最终一致性模型)
* [3 [Time and order](http://book.mixu.net/distsys/time.html)](#3-[time-and-order](http://bookmixunet/distsys/timehtml))
	* [3.1 全局有序、部分有序](#31-全局有序、部分有序)
	* [3.2 什么是“时间”？](#32-什么是“时间”？)
	* [3.3 各处的时间都以同样的速率前进吗？](#33-各处的时间都以同样的速率前进吗？)
		* [3.3.1 time with a "global clock" assumption](#331-time-with-a-"global-clock"-assumption)
		* [3.3.2 time with a "local clock" assumption](#332-time-with-a-"local-clock"-assumption)
		* [3.3.3 time with a "no clock" assumption](#333-time-with-a-"no-clock"-assumption)
	* [3.4 分布式系统中的“时间”是如何应用的？](#34-分布式系统中的“时间”是如何应用的？)
		* [3.4.1 时钟向量（vector clocks, time for causal order）](#341-时钟向量（vector-clocks,-time-for-causal-order）)
		* [3.4.2 失败检测（failure detectors，time for cutoff）](#342-失败检测（failure-detectors，time-for-cutoff）)
		* [3.4.3 时间、顺序、性能（time，order and performance）](#343-时间、顺序、性能（time，order-and-performance）)
* [4 [Replication: preventing divergence](http://book.mixu.net/distsys/replication.html)](#4-[replication:-preventing-divergence](http://bookmixunet/distsys/replicationhtml))
* [5 [Replication: accepting divergence](http://book.mixu.net/distsys/eventual.html)](#5-[replication:-accepting-divergence](http://bookmixunet/distsys/eventualhtml))
* [6 [Appendix](http://book.mixu.net/distsys/appendix.html)](#6-[appendix](http://bookmixunet/distsys/appendixhtml))


该系列文章[Distributed Systems for Fun and Profit](http://book.mixu.net/distsys/)主要是介绍了分布式系统编程以及分布式系统的**核心概念**，方便读者后续针对性的深入了解。如果要包含分布式系统的方方面面，那是一件非常疯狂、工作量巨大的事情。

# 1 [分布式系统高层视角](http://book.mixu.net/distsys/intro.html)

## 1.1 水平扩展 vs 垂直扩展

每一个计算机系统都需要关注两方面的事情：**“计算” + “存储”**。
    
假如我们有足够的资金，当计算机硬件能力跟不上时，我们可以考虑升级不停地升级硬件（**垂直扩展，vertical scaling**），甚至是雇佣专门的团队来帮我们设计计算机，就不存在分布式系统的问题了。但我们的资源是有限的，技术发展是有瓶颈的，不可能一直对计算机做升级。

分布式系统，通过构建集群，通过添加新节点的方式来提升整体的处理能力（**水平扩展，horizontal scaling**）。水平扩展才是真正比较可行的解决方案。而且，集群中节点数量增多之后，节点之间的通信也会增多，这里的通信开销也会影响计算效率。所以需要研究高效的分布式算法来优化。

## 1.2 分布式系统追求什么

- Scalability
    
    > is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth.
    
    困难大小源于问题规模。如果要处理的问题规模很小，那困难是微不足道的，比如让“举起”一块巧克力，但是让举起一座山，那就很困难了。当问题规模超过了一定的尺寸、容量或者物理条件限制时，解决问题就会变得非常困难。

    所以说，**everything starts with size - scalability**。在一个设计良好的分布式系统中，系统处理不会因为问题规模从小渐渐变大就变得很糟糕。

    **问题规模增长，指的都是哪些维度的增长**？有3个令人感兴趣的维度供参考。
    - Size scalability：系统集群中添加新节点会线性提升集群计算能力，待处理数据集、请求量增大后并不会明显增加系统处理时延；
    - Geographic scalability： 地理上会使用多地多机房的部署，如在天津、上海、深圳都有部署数据中心，来减少不同地域用户的请求时延。某些时延要求敏感场景下，也需要优化流量调度来减少跨城时延；
    - Administrative scalability：系统集群中添加更多节点不会明显引入额外的管理运维代价； 

    当然在真实的系统中，随着问题规模扩大，系统的“**growth**”是多维度的，不止上面这些。
    
- Performance (and latency)

    > is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used.

     一个可扩展的系统随着规模扩大，依然能够满足用户需求。其中有两个比较重要的指标：performance 和 availability，这两个指标有各种各样的度量方式。关于performance，结合具体上下文，它可能代表了下面几个含义：

     - 针对特定的任务处理，有较短的响应时间（response time）或处理时延（low latency）；
     - 针对特定的任务类型，有较高的吞吐量（high throughput）或较快的处理速率（rate of processing work）；
     - 较低的资源占用率（low utilization of computing resources），可能隐含了更优的算法、逻辑；
     
     针对这些指标对系统做优化时，涉及到一系列的权衡、折中（tradeoffs）。比如，系统可以通过处理较大型的作业来提升系统吞吐量，作业较小可能意味着人为的干预、会中断系统处理，但是较大型的作业的处理结束的响应时间也会变长，那这里在响应时间和吞吐量之间就做了折中。

     处理时延latency的含义，与响应时间差不太多：

     > The state of being latent; delay, a period between the initiation of something and the occurrence.

     在分布式系统中，latency不可能为0，如信息需要通过电信号传输，速度可以认为是光速，看上去很快，latency可以忽略不计？No，硬件有自己的物理特性，硬件操作都是有时延的，尽管看起来很小，但是这些时延会叠加起来……所以latency不可能为0或忽略不计。最终latency的大小，取决于任务处理的工作量大小以及信息传输的距离。
   

- Availability (and fault tolerance)

    > the proportion of time a system is in a functioning condition. If a user cannot access the system, it is said to be unavailable.

    分布式系统使得我们可以获得单机系统没有的特性，如容错。单机部署的服务如果挂掉了，是没法容错的，只能事后重启来应对。
    
    分布式系统中，其实应用了很多不可靠的组件，但是将这些组件结合起来却可以构建一个可靠的分布式系统。系统通过冗余（redundancy）可以实现部分容错，可以获得更好的可用性（availability）。

    冗余（redundancy），结合上下文，也可以指代不同的东西，如组件的冗余、server的冗余、数据中心的冗余、网络的冗余，等等。

    可用性的计算公式，很简单，`availability = uptime / (uptime + downtime)`，业界通常用“几个9”来表示可用性级别，通过不同可用性级别对应的每年停机时间，会获得一个非常直观的感受：

    |可用性级别     |每年停机/故障时间|
    |---------------|-----------------|
    |90%，1个9      |超过1个月        |
    |99%，2个9      |不足4天          |
    |99.9%，3个9    |不超过9个小时    |
    |99.99%，4个9   |不超过1个小时    |
    |99.999%，5个9  |不超过5分钟      |
    |99.9999%，6个9 |不超过31秒       |

    可用性的概念，不能仅仅停留于系统上线时间（uptime），更精准地描述，应该是系统能否正常工作、满足用户请求。这样的话要求就更高了，因为涉及到了响应时间等具体的硬指标。

    一个系统的可用性要达标，需要考虑方方面面的事情，系统走的网络、数据中心、应用的组件、本身部署的机房、依赖的外部API，等等，因为不能假定这些things都是可靠、不可能出问题的，所以在设计的时候最好就要考虑到冗余设计，来完成部分容错，不然不可能获得一个高可用的分布式系统。

    那么，容错（fault tolerance）指的是什么呢：

    > ability of a system to behave in a well-defined manner once faults occur

    要实现容错，前提是要知道错误或者故障的类型，然后才能设计对应的系统或者提出一种算法来在遇到该错误时进行处理、补偿。不可能对未考虑过的错误进行自动容错。

## 1.3 上述目标为何难以实现

分布式系统，受限于两个物理上的因素：

- 系统中节点的数量（节点数量增加，提高了系统的存储容量和计算能力）；
- 节点之间的物理距离（信息传输需要时间，最好情况下是光速传输，但是硬件自己的电器特性还是有开销的）；

在这些约束基础上，我们再来看下：

- 系统中节点的数量增加，也意味着节点出现故障的概率更大了（任一节点正常工作概率为p (p<0)，则系统中出现故障节点概率为1-p^n），会降低可用性、增加运维成本；
- 系统中节点的数量增加，也增加了节点之间的通信开销（节点要与其他节点通信，通信请求数量多了），随着集群规模扩大，通信引入的开销会降低节点的计算性能；
- 系统中节点的数量增加，如果节点间地理位置增大了，节点间最小通信时延会增加，会增加系统处理或特定操作的响应时间；

    ![cluster size (number of cores)](http://book.mixu.net/distsys/images/barroso_holzle.png)

分布式系统设计，需要先确定设计目标，设计实现时需要为了设计目标来做各种现实的权衡、折中。通常这里的设计目标，需要以各种保证条款的形式沉淀下来，如SLA（Service Level Agreement）。如，如果一次数据写操作成功，隔多久可以从其他地方访问最新数据？或者数据写入之后，数据持久性能保持多久？或者针对一次计算，隔多久可以计算完成返回结果？再或者说，如果一个组件失败，或者某个操作出问题，对系统会造成多大范围的影响、事故？

系统是代替人干活的，所以还有另一个层面的问题，系统的可理解性（intelligibility）。系统处理的结果、给出的信息、告警信息、错误信息等等是不是易于理解？有多么容易让人理解？

我们见过设计的很糟糕的系统，出点问题，经常需要拉起很多人、用很多时间、经过多番讨论、监控数据对比才能定位到是哪个server发生了问题、问题又是如何传播引起了整个系统的故障。对可理解性方面，没有明确的一些metrics可供度量、列出，但是它依旧是一个非常重要的维度。还有就是，一个系统的运行状态是否正常，是否能够做出判断，是否一定需要有经验的人才能做出诊断。这也是一些可以使用的metrics。

## 1.4 抽象 & 建模

前面提到的这些问题，就需要通过一定的抽象和建模来解决。

- 抽象（abstractions），指的是对要解决的一些具体的问题，做一些褪去无关细节、提升思考维度做些通用性设计，以解决一类问题的方法；
- 建模（models），以更精确的方式来描述了一个分布式系统应该具备的各种关键属性；

    - System model (asynchronous / synchronous)
    - Failure model (crash-fail, partitions, Byzantine)
    - Consistency model (strong, eventual)

    ps: 关于建模的部分，请见本文下面的描述。

良好的抽象使系统更容易理解，同时能够捕获与特定问题相关的各种影响因素。

分布式系统中包含了很多节点，我们希望分布式系统能像一个单机系统那样易于理解，在这之间就存在一点冲突。比如，我们希望在一个分布式系统上实现一个共享内存的抽象设计，共享内存虽好理解，但是实现基于分布式系统实现却很复杂。这样看来，常常我们单机系统中熟悉的模型，在分布式系统中实现起来会很复杂。

一个系统，如果对各种保证方面有更弱的保证，通常会获得更大的自由度，也会因此而获得更好的系统性能，但另一方面，也会变得更难于理解。我们很容易理解一个单机系统中发生的事情，但是对于包含很多节点的分布式系统，要说清楚其中发生的事件则相对困难，尤其是一些异常、错误。

人们通常可以通过公开有关系统内部的更多细节来获得性能。 例如，在列式存储中，用户可以（在某种程度上）推断键值对在系统中的位置，从而做出影响典型查询性能的决策。 隐藏这些细节的系统更易于理解（因为它们的行为更像是一个单元，更少的细节需要考虑），而暴露更多真实细节的系统则可能更具性能（因为它们与现实的关系更紧密） 。

个别类型的故障使实现像单机系统一样工作的分布式系统变得非常困难。网络时延（network latency）和网络分区（network partition，total network failures between some nodes）意味着当系统中出现这类故障时，系统有时需要做出艰难的选择，以便更好地保持可用性，但失去了一些无法强制执行的关键保证，或者为了其安全性就拒绝了客户端请求。

下文中会提及CAP定理，来进一步解释这里遇到的一些问题。最后，理想的系统既可以满足程序员的需求（清晰的语义）又可以满足业务的需求（可用性、一致性、延迟等）。

## 1.5 设计技巧：“分区”和“复制”

分布式系统中有很多节点，节点要做计算就需要先知道待计算的数据是如何存储的、存储在哪里，数据集是如何存储的对分布式系统而言是非常重要的。

对于数据集存储，通常有两种方式：

- 分区（partition），数据集可以根据一定的规则做适当的分割（split）然后存储到不同的节点上；

    > is dividing the dataset into smaller distinct independent sets; this is used to reduce the impact of dataset growth since each partition is a subset of the data.

    分区把数据集按照特定split规则进行分割，如按照用户uid将不同用户信息切分开，然后存储到不同的节点上。相比完整数据集来讲，单一分区上的数据量大幅减小，有助于实现数据的最终存储，以及后续数据的高效查询。

    即便后续有的节点出现了失败，也只会影响该节点上存储的分区，并不会影响到整个的数据集，其他节点上存储的分区依然是可用的。并且这里出问题的节点，我们也可以进一步通过复制来增强可用性。    
    分区，和具体的数据集分割的规则有关系，和应用场景有关系的，这里就先不多说了，我们将重点关注“复制”。
     
- 复制（replication），数据集也可以被复制（copied）或缓存（cached）到不同的节点上，以减少为了访问其他节点存储的数据引入的通信时延，也可以实现更好的容错，避免节点故障引入的数据丢失；
    
    > copying or reproducing something - is the primary way in which we can fight latency.

    复制，为数据生成了额外的副本，提高了容错能力，提高了系统的可用性。存储这些数据副本的节点也可以用来对外提供服务，也提升了系统的整体性能、降低处理时延（latency）。复制，提供了额外的带宽（bandwidth），复制在延时敏感领域可以通过缓存降低数据访问时延。复制，和维护数据一致性也息息相关（如WAL，Write Ahead Log）。

    复制，允许我们获得更好的扩展性、性能、容错能力，有了复制，就不用那么担心可用性受损或者数据访问性能下降的问题，可以通过复制数据到其他节点来降低系统访问瓶颈和数据单点故障。复制的概念可以进一步延伸，如复制计算，担心计算性能问题，将计算复制到多个系统，采用分布式计算的方式。担心慢速IO，将数据复制到本地cache来降低延时，或者复制到多机来提升数据访问的吞吐量。

    另外，也要意识到，复制也是造成一些难解问题的原因，比如数据一致性问题。数据复制到多个节点，如何维护这些数据副本的一致性，在分布式领域是一个非常重要的问题，**数据一致性模型**的选择对维护数据一致性至关重要。好的一致性模型即具有清晰的语义（如方便理解）也能满足业务要求（如数据强一致、高可用等）。
    
    对于复制，只有强一致模型才能够像对待没有数据复制一样轻松地编码实现。其他一致性模型，都或多或少暴露了一些数据复制的细节信息给开发人员，开发人员需要理解这样的复制存在的问题，如数据不总是一致的，需要借助一些其他手段来识别数据是否ok，编码上就没那么简单。但是，弱一致性模型，因为约束少、自由度大，往往可以获得更好的系统性能、更高的可用性，当然也并不是那么难以理解。
    
    业界存在一些经过验证的一致性算法如Paxos、Raft、ZAB等等，这些一致性算法既需要考虑如何维护数据一致性，也需要考虑降低维护一致性的开销、保证分布式系统的处理性能。

下图中展示了分区、复制二者之间的一个区别，图中数据集A和B是以分区的形式存储到了不同节点Node1和Node2上，数据集C则是以复制的方式在Node1、Node2上进行了存储。

![parition and replicated](http://book.mixu.net/distsys/images/part-repl.png)

不同的分布式系统可能会选择不同的数据存储策略，可能是分区，也可能是复制，更常见的是两者的集合，数据集会分区存储到不同节点，同时也会在其他节点生成数据的副本。现在需要了解的是，分区、复制这两种方式，它们各有优缺点，分区、复制也有不同的算法来支持，需要结合具体的设计目标来选择和评估。


参考资料：
- [The Datacenter as a Computer - An Introduction to the Design of Warehouse-Scale Machines](http://www.morganclaypool.com/doi/pdf/10.2200/s00193ed1v01y200905cac006) - Barroso & Hölzle, 2008
- [Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing)
- [Notes on Distributed Systems for Young Bloods](http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/) - Hodges, 2013

# 2 [关注不同抽象层次](http://book.mixu.net/distsys/abstractions.html)

我们先在不同抽象层次上观察，来看一些“**不可能的结果**”（impossibility results，如CAP和FLP），然后再把注意力转向更具体的层次来关注下“性能”。

经常编程的话，应该熟悉“抽象层次”的提法。开发人员通常工作在某个抽象层次上，并且通过API调用与更下层进行交互，并且当前抽象层次也需要提供API给更上层的应用调用或者与用户交互。七层OSI网络模型便是一个很好的例子。

分布式编程很大程度上是在处理“分布"的问题。我们的系统中有很多的节点，但我们希望它能像一个单机系统一样工作，这两者之间是存在矛盾的。意味着，我们需要找到一个好的抽象，以在系统的可能性、可理解性和性能之间达到一个平衡。

抽象层次中，当我们说X比Y更抽象时，是什么意思呢？首先，X不会引入任何新的或与Y根本不同的东西。实际上，X可能会删除Y才有的某些东西或从更易于管理的角度出发来进行一般化设计。其次，X移走的某些Y的东西对于手头的事情并不重要，X比Y更容易掌握。抽象，更简单的问题描述，褪去了各种无关的具体细节、又保留了本质，更容易被分析，提出的解决方案也更具有普适性。

实际上，如果我们能够保留一些问题的本质，那么结果、派生结果也会具有相对广泛的应用。这就是不可能结果问题（impossiblility results）非常重要的原因：它们对问题进行了简化，基本上是用最简单的问题形式，来揭示即便是施加了一些约束、限制、假设后，我们仍然不可能解决该问题。从而针对该问题的更一般化的问题，可以得出我们更无法解决的结论。

所有的抽象都忽略了一些东西，抽象的诀窍就是扔掉所有不必要的东西。你怎么知道什么是最重要的呢？嗯，你可能不会事先知道。

每次我们从系统规范中排除系统的某些方面时，都会冒引入错误和性能问题的风险。这就是为什么有时我们需要朝另一个方向前进，并有选择地引入真实硬件和真实世界问题的某些方面。重新引入某些特定的硬件特性（例如，物理顺序）或其他物理特性，可能足以使系统运行良好。

对于分布式系统时，抽象设计时，需要保留的最少现实信息的度如何把握呢？系统模型（system model），是这方面的一个重要规范。指定了一个系统模型之后，我们可以进一步看看相关的不可能结果和面临的挑战。

## 2.1 System Model（系统模型）

> a set of assumptions about the environment and facilities on which a distributed system is implemented.

分布式系统的一个关键属性就是“分布式”，分布式系统中的程序：

- 在众多相互独立的节点上并发运行；
- 众多的节点（或程序）之间通过网络进行互联、通信，会引入一定的不确定性和消息丢失；
- ...
- 没有共享内存；
- 没有共享时钟；

上面这些问题，暗示着：

- 分布式系统中的每个节点以并发的形式运行程序；
- 分布式系统中的每个节点掌握的信息都是偏向于本地（或局部）的：节点可以快速节点本地的状态信息，但是访问全局状态信息则很可能获取到的是过时的信息；
- 分布式系统中的每个节点都可能出现失败，也可能从失败中恢复，这些情况都是可能相互独立出现的；
- 分布式系统中的消息可能出现延迟送达或者丢失的情况（节点失败具有独立性，区分网络失败或者节点失败并不是一件很简单的事情）；
- 分布式系统中各个节点的时钟可能出现不同步（一个节点的本地时间戳，不一定总能代表全局的时间戳，多个节点执行操作顺序，用各个节点的本地时间戳来排序从系统全局的时间戳来看可能是排序错误的）；

一个系统模型（system model），需要列举与特定系统设计相关的许多假设。不同的系统模型在关于环境（environment）和设施（facilities）的假设方面会有所不同。这些假设包括：

- 节点要实现哪些功能，以及会如何失败；
- 节点之间的通信链路是怎样的，以及会如何失败；
- 系统整体的一些属性，如关于时间和顺序（time and order）的假设；

一个健壮的系统模型，提出的假设在约束性方面应该尽可能弱：这样设计出来的算法也更健壮，能够容忍系统在各种不同的环境中运行，因为它做少了更少、更弱的假设。

另一方面，我们可以通过做出强有力的假设来创建易于推理的系统模型。 例如，假设节点没有故障意味着我们的算法不需要处理节点故障。 但是，这样的系统模型是不现实的，因此难以应用于实践。

让我们更详细地看一下节点的属性、通信链路以及时间和顺序。

## 2.2 系统模型中的节点

系统中的节点，充当计算、存储用的主机，节点通常具备：

- 执行程序的能力；
- 存储数据到易失性存储器、持久化存储器（节点失败后也可以从中读取数据）的能力；
- 拥有一个时钟（可能不准确，并且要假定它可能不准确）；

节点执行的是确定性的算法：执行本地计算，存储计算后的本地状态，发送出去的消息由接收到的消息以及当时的本地状态唯一确定。

有许多可能的故障模型（failure models）描述了节点可能出现故障的方式。 实际上，大多数系统都采用崩溃恢复故障模型（crash-recovery failure model）：也就是说，节点只能通过崩溃来发生故障，并且（可能）可以在崩溃以后的某个时刻恢复。

另一种选择是假设节点可以通过任意的错误行为而出现故障、失败，这种称为拜占庭故障（Byzantine faults）。拜占庭故障在现实世界的商业系统中是很少处理的，因为能够抵抗任意故障的算法运行起来会很复杂、代价高、实现也复杂。我们这里不会讨论这个，可以了解下Paxos或者Raft一致性算法。

## 2.3 系统模型中的通信链路

通信链路将一个节点和其他节点连接在一起，并且允许在连接的节点之间互相传递消息。很多讨论分布式算法的书中，假定系统中的任意两个节点之间都是有独立的链路连接的，并且提供FIFO有序的消息传递，这里发送的消息可能会出现丢失的情况。

有些算法假定网络是可靠的：链路上传递上的消息永远不会出现丢失、延迟送达。在真实世界中的某些场景下，这种假设可能是成立的，但是对绝大部分场景而言，还是假设网络不可靠比较好，尤其要考虑网络中的消息丢失和延迟的影响。

如果系统中出现了网络失败，就可能会形成网络分区，分区中的节点还是可以保持工作，只是不同分区中的节点不能相互通信了。当网络故障后出现分区时，分区之间的消息传递可能会出现丢失、延迟，直到网络分区被修复。不同分区中的节点仍然可以被某些clients访问，分区中的节点也可以提供服务，这与节点crash后无法提供服务是两种明显不同的情况，必须要对这两种情况进行区分对待。下图显示了节点失败和网络分区的故障发生点的不同，前者是节点，后者是网络：

![node failure vs. network partition](http://book.mixu.net/distsys/images/system-of-2.png)

很少会对通信链路做一些假设，我们可以假设通信链路是单向传输的，或者我们可以引入对不同通信链路的通信开销的考量（如不同物理距离的通信时延）。但是，在真实的商业系统中需要做这种假设的情况比较少，除非涉及到较远距离的通信（如广域网WAN通信），所以这里也不会做过多假设。成本和拓扑的更详细模型可以以复杂性为代价进行更好的优化。

# 2.4 时间/顺序假设

分布式系统中节点的不同物理分布带来的一个问题是，每个节点对这个世界的“感受”都是不同的。如果节点彼此之间的距离不同，则从一个节点发送到另一个节点的信息将在不同的时间到达，并且可能以不同的顺序到达其他节点。

时间假设在不同的系统模型里面也是不同的，我们主要关注的两个模型：

- 同步系统模型（synchronous system model）

    > Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock.

    同步系统模型对时间和顺序施加了许多约束。它基本上假设节点具有相同的体验：发送的消息始终能在最大传输延迟内被接收，并且过程以lockstep方式执行。这很方便，因为它使您可以对时间和顺序进行假设，而异步系统模型则不能。

- 异步系统模型（asynchronous system model）

    > No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist.

    异步模型中，您不能依赖定时（或“时间传感器”）。

    解决同步系统模型中的问题更容易，因为相关的假设（如执行速度、最大消息传输延迟和时钟准确性）都有助于解决问题，因为您可以基于这些假设进行推断来排除一些故障。

    当然，同步系统模型在现实中并非总是成立。现实世界的网络容易出现故障，并且对消息延迟没有严格的限制。现实世界的系统充其量只能是部分同步模型：它们有时可能会正常工作并提供一些消息传输时延、始终准确性的上限，但是有时消息会无限期地延迟并且时钟不同步。我不会在这里讨论同步系统的算法，但是您可能会在许多其他入门书中碰到它们，因为它们在分析上比较容易。

参考资料：

- [lockstep (computing)](https://en.wikipedia.org/wiki/Lockstep_(computing))

## 2.5 共识问题（Consensus Problem）

接下来我们将更改系统模型的参数，再接下来，我们将研究改变两个系统属性（**网络分区、同步或异步时序**）之后将如何影响系统设计。

- 系统模型中是否考虑了网络分区问题；
- 系统模型中是否考虑了同步、异步时序问题；

讨论上述问题如何影响系统设计，需要考虑两个**不可能的结果（impossibility results），FLP以及CAP**。

当然，为了展开讨论，我们需要引入一个问题，在尝试解决的过程中展开讨论，这里引入的问题是共识问题（consensus problem）。

当多台计算机（或者多个节点）能够在一些“值”上面达成一致，就说它们达成了共识。

“**共识问题**”更正式地描述，可以归纳为：

Agreement：每个正确的进程，都必须商定相同的“值”；
Integrity：每个正确的进程，最多能决定一个“值”，如果系统最终商定了一个值，这个值一定是由某个（或某些）进程确定的；
Termination：所有进程最终都必须能做出决定；
Validity：如果所有正确的进程都提出了相同的值V，那么所有正确的进程都将决定值使用值V在值V；

共识问题，是很多商用分布式系统的核心。毕竟，我们希望系统能够具备分布式系统的可靠性和性能的同时，能够不用去处理琐碎的“分布式”的问题（例如，节点之间的分歧），解决共识问题可以解决一些相关的更高级的问题，例如原子广播（atomic broadcast）和原子提交（atomic commit）。

## 2.6 Two impossibility results

第一个不可变结果问题，是FLP不可变结果（FLP impossibility result），该不可变结果用来指导分布式算法的设计人员。

    >FLP不可变结果：在一个完全异步的环境中，不可能找到有效的共识算法。

第二个不可变结果问题，是CAP理论，该不可变结果更倾向于出现网络分区时对系统设计人员的一些指导，而不是对分布式算法人员的直接指导。

    >CAP定理：在一致性（Consistency）、可用性（Availability）、分区耐受性（Partition Tolerance）三者中，分布式系统只能三选二。

### 2.6.1 The FLP impossibility result

FLP不可能结果，在分布式相关的学术研究中FLP不可能及其证明具有非常重要的地位，感兴趣的可以参考 /materials/A Brief Tour of FLP Impossibility.pdf，本文这里只对FLP不可能结果进行概述。FLP不可能结果是以3名研究人员姓名命名的（分别是Fischer、Lynch和Patterson）。FLP不可能结果，研究了异步模型下的共识问题（技术上是agreement问题，是共识问题的一种非常弱的形式）。FLP不可能结果证明论文中，做了这样的假设：假定节点只能因崩溃而失败；假定网络是可靠的，并且异步系统模型的典型时序假设成立，即对消息延迟没有任何的约束、限制。

在这些假设下，FLP不可能结果表明：“在一个遭受故障的异步系统中，不存在一个确定性的算法能够用来解决共识问题，即使系统中永远不丢失消息、最多一个进程失败（且因为崩溃失败，崩溃就是停止执行）”。

这个结果意味着，即使在一个非常小的系统模型中，不可能以一种永远没有延迟的方式解决共识问题。论据是，如果存在这样的算法，则可以设计该算法的执行，其中该算法将通过延迟消息传递而在任意时刻保持不确定状态（“二价”，bivalent），这在异步系统模型中是允许的。这与假设冲突，因此这种算法不存在。详细的FLP不可能结果证明，可以参考 /materials/A Brief Tour FLP Impossibility.pdf。

**FLP不可能结果很重要，因为它强调了对异步系统模型不得不做这样一个折衷：当消息传递没有保证时，解决共识问题的算法必须放弃安全性（safety）或者放弃活性（liveness）**。

那么，如何理解safety和liveness呢？

首先，一个并发程序的执行过程可以看做是一个无穷的状态序列，下面是safety和liveness的非正式定义：

- safety：safety属性指的是，一些'bad thing'在程序执行期间不发生。safety属性相关的示例有互斥、消除死锁、部分成功、先来先服务；
- liveness：liveness属性指的是，一些'good thing'在程序执行期间能发生。liveness属性相关的示例包括避免饿死、算法正常终止、确保服务；

其他的属性，其实可以说是safety（避免bad thing发生）、liveness（让good thing发生）的一些取交操作衍生而来。


FLP不可能问题，或者说结论，与算法设计人员特别相关，因为它对异步模型中我们可以解决的问题添加了严格的约束。

ps：消息传递在异步系统中，几乎总是不可能保证的，所以解决共识问题的算法只能在安全性或可用性之间做取舍。我认为是FLP不可能结果基础上，进一步催生了CAP理论。CAP定理是与从业人员更相关的一个相关定理：它做了与FLP稍微不同的假设（假设存在网络故障而不是节点故障），并且对从业人员在系统设计中进行取舍具有更明显的含义。

参考资料：
- [A Brief Tour of FLP Impossibility.pdf](https://www.the-paper-trail.org/post/2008-08-13-a-brief-tour-of-flp-impossibility)
- [Impossibility of distributed consensus with one faulty process](http://scholar.google.com/scholar?q=Impossibility+of+distributed+consensus+with+one+faulty+process) - Fischer, Lynch and Patterson, 1985
- [Defining liveness](https://www.sciencedirect.com/science/article/abs/pii/0020019085900560?via%3Dihub) - Bowen Alpern, Fred B. Schneider


### 2.6.2 The CAP Theroem

    > a **deterministic** consensus protocol cannot have **liveness**, **safety** and **fault tolerance** in a fully asynchronous system.

CAP定理最初是由计算机科学家Eric Brewer提出的一个猜想。 在系统设计的保证中考虑折衷是一种流行且相当有用的方法。它甚至有Gilbert和Lynch的正式证明。

该定理说明了这三个属性：

- 一致性：所有节点同时看到相同的数据；
- 可用性：节点故障不会阻止幸存者继续运行；
- 分区耐受性：网络和节点故障会导致消息丢失，这种情况下系统仍可继续运行；

CAP理论表明，我们同时只能满足两个条件。我们甚至可以将其绘制成漂亮的图，从三个属性中选择两个属性，可以得到对应于不同交集的三种类型的系统：

![CAP三选二可得到的3种系统](http://book.mixu.net/distsys/images/CAP.png)

请注意，该定理指出中间区域对应的系统（具有所有三个属性）是无法实现的。 然后我们得到三种不同的系统类型：

- CA（consistency + availability），相关的实例如一些很严格的协议，例如两阶段提交；
- CP（consistency + partition tolerance），相关的示例包括多数协议（协议中假定少数分区不可用），例如Paxos；
- AP（availability + partition tolerance），相关的示例如一些依赖冲突解决的协议，如dynamo；

CA和CP系统设计均提供相同的一致性模型：高度一致性。 唯一的区别是CA系统不能容忍任何节点故障。 在非拜占庭式故障模型中，CP系统最多可以容忍给定2f + 1个节点的f个故障（换句话说，只要多数f + 1处于故障状态，CP系统就可以容忍少数f个节点的故障）。 原因很简单：

- CA系统无法区分节点故障和网络故障，因此必须停止在所有地方接受写入，以避免引入差异（多个副本）。 它无法确定远程节点是否已关闭，或者仅网络连接是否已关闭：因此唯一安全的事情是停止接受写入。
- CP系统通过在分区的两侧强制非对称行为来防止不一致（例如，保持单副本一致性）。 它仅保留多数分区，并要求少数分区不可用（例如，停止接受写入），这保留了一定程度的可用性（多数分区），并且仍确保单副本一致性。

当我讨论Paxos时，我将在复制一章中对此进行更详细的讨论。 重要的是，CP系统将网络分区纳入到其故障模型中，并使用Paxos、Raft或viewstamped一致性算法来区分多数分区和少数分区。CA系统不支持分区，并且在历史上更为常见：它们通常使用两阶段提交算法，并且在传统的分布式关系数据库中很常见。

假设发生分区，则该定理简化为可用性和一致性之间的二元选择。

![CAP如果分区必然发生](http://book.mixu.net/distsys/images/CAP_choice.png)

我认为应该从CAP定理中得出四个结论：

- 首先，早期分布式关系数据库系统中使用的许多系统设计都没有考虑分区耐受性（例如，它们是CA设计）；

  分区耐受性是现代系统的一项很重要的属性，因为网络分区是很可能出现的，对于一些多地部署的大型系统更是这样。

- 其次，在网络分区过程中，强一致性和高可用性之间存在冲突关系；

  CAP理论概述了在强一致和分布式计算之间进行取舍的情况。某种意义上，保证一个分布式系统与非分布式系统没有任何区别是非常疯狂的，因为分布式系统往往是由不可靠网络连接很多独立节点而构成的，有很多问题需要尝试去解决。

  强一致性保证，要求在出现网络分区时放弃可用性，为什么呢，因为在当两个副本（replicas）之间不能正常通信时，这些副本可能还在接收不同的写请求，这样的话，我们是不能阻止它们出现数据不一致的。
  如何解决这个问题呢？可以对假设进行加强（假定不会出现分区）或者弱化这里的一致性保证。可以拿一致性和可用性（在线服务、低延时……）做个权衡、折中。通常强一致性可以定义成：“所有的节点在同一时刻看到的同一个数据的值是相同的”，如果系统中对一致性的定义要比这个弱一些的话，那我们就可以在可用性和（相对弱的）一致性之间做点折中。

- 第三，在正常操作中，强一致性和性能之间存在冲突；

  强一致性/单副本一致性，要求节点进行通信并就每个操作达成一致，这会导致正常操作期间的高延迟。如果您可以使用经典模型以外的一致性模型（允许副本滞后或暂时不一致）的一致性模型，则可以减少正常操作期间的延迟并在存在分区的情况下保持可用性。

  当涉及较少的消息和较少的节点时，操作可以更快地完成。但是实现此目标的唯一方法是放宽保证：让某些节点的联系频率降低，这意味着节点可以包含旧数据。

  这也使得可能发生异常。您不再保证获得最新的值。根据做出何种保证，您可能读取的值比预期的要旧，甚至丢失一些更新。

- 第四，间接得出，如果我们不想在网络分区期间放弃可用性，那么我们需要研究强一致性以外的一致性模型是否对我们的目的可行。

  例如，即使将用户数据地理复制到多个数据中心，并且这两个数据中心之间的网络连接暂时出现故障，在许多情况下，我们仍然希望允许用户使用网站/服务。这意味着以后需要协调两组不同的数据，这既是技术挑战，又是业务风险。但是技术挑战和业务风险通常都是可控的，因此最好提供高可用性。

  一致性和可用性并不是真正的二选一问题，除非要实现强一致性。但是强一致性只是一致性模型中的一种：在这种模型中必须放弃可用性模型，以防止激活多个数据副本。

  如果您仅从本次讨论中删除一个想法，那就这样吧：“一致性”不是一个孤立的、明确的属性。 记住：[ACID](https://en.wikipedia.org/wiki/ACID) consistency != [CAP](https://en.wikipedia.org/wiki/CAP_theorem) consistency != [Oatmeal](https://en.wikipedia.org/wiki/Oatmeal) consistency。
  
  相反，**一致性模型是数据存储给使用它的程序的保证**-任何保证。

## 2.7 一致性模型（consistency model）

  > consistency model: a contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictable.

CAP中的“C”指的是“强一致性”，但是“consistency”并不是“强一致性”的同义词，需要明白这点。

### 2.7.1 强一致性模型 VS. 其它一致性模型

一致性模型可以分为两类：**强一致性模型** 和 **弱一致性模型**：

- 强一致性模型（strong consistency models）维护一个副本
    - linearizable consistency
    - sequential consistency
- 弱一致性模型（weak consistency models）
    - client-centric consistency models
    - causal consistency: strongest model available
    - eventual consistency models

强一致性模型，保证更新的明显顺序和可见性，与一个非复制的系统完全一致。弱一致性模型，则不做这种保证。

请注意，这绝不是详尽的一致性模型的清单。一致性模型只是程序员和系统之间的契约，因此它们几乎可以是任何东西。

### 2.7.2 强一致性模型

强一致性模型可以进一步划分为两类，它们有点相似但又有一点不同：

- 线性一致性（linearizable consistency）：在线性化一致性下，所有操作似乎都按照与操作的全局实时顺序一致的顺序自动执行。 （Herlihy＆Wing，1991）
- 顺序一致性（sequential consistency）：在顺序一致性下，所有操作似乎都以某种顺序原子执行，该顺序与单个节点上看到的顺序一致，并且在所有节点上都相等。 （Lamport，1979年）

关键区别在于，线性化一致性要求操作生效的顺序等于操作的实际实时顺序。顺序一致性允许对操作进行重新排序，只要在每个节点上观察到的顺序保持一致即可。有人可以区分两者的唯一方法是，他们可以观察进入系统的所有输入和时序。从客户端与节点交互的角度来看，两者是等效的。

差异似乎无关紧要，但是值得注意的是顺序一致性并没有组成（doesn't compose）。

强一致性模型允许开发人员将访问一个单节点服务轻松替换为访问一个分布式多节点构成的集群，而不会引发任何问题。

所有其他一致性模型都会引发异常（与保证强一致性的系统相比），因为它们的行为方式与非复制系统不同。但是这些异常通常是可以接受的，或者是因为我们不关心偶发性问题，或者是因为我们编写了处理不一致问题的代码。

请注意，对于弱一致性模型，实际上并没有任何通用类型，因为“不是强一致性模型”（例如“以某种方式可与非复制系统区分开”）几乎可以是任何东西。

### 2.7.3 以client为中心的一致性模型

以client为中心的一致性模型，指的是这样的一致性模型：它以某种方式围绕客户端或者会话（session）展开。例如，系统会保证客户端不会看到比当前看到的数据更旧版本的数据。通常可以在client library中增设cache，如果访问后端返回的数据版本比当前缓存的数据更旧，那么直接返回缓存的数据即可，而不是副本中更旧的数据。

这种情况下，client如果初始连接到了一个存有旧数据的副本，获取并缓存了旧版本的数据，这种情况下client也会看到旧数据，但是当它后续访问时，不会访问到比这个版本更旧的数据。如果缓存过期，或者异步更新缓存时发现了副本中更新版本的数据，则可以更新client缓存的数据。

有很多类型的一致性模型，属于以client为中心的一致性模型。

### 2.7.4 最终一致性模型

最终一致性模型，意思是说，如果我们停止更新某个值，过一段时间t之后，所有的副本将达成一致更新成相同的值。暗示，在这个时间t到达之前，各个副本中的数据会出现不一致，而且不一致的方式是不确定的。由于它是微不足道的（仅活泼性），因此没有补充信息就没有用。

说某件事最终最终是一致的，就像说“人们最终死了”。 这是一个非常弱的约束，我们可能希望至少对两件事进行更具体的描述：

- 首先，“最终一致”这里的“最终”会持续多长时间？应该至少了解系统收敛到相同值通常需要多长时间，或者应该有个严格的下限。
- 其次，副本如何在一个价值上达成共识？始终返回“ 42”的系统最终是一致的：所有副本都同意相同的值。它只是不会收敛到有用的值，因为它只会不断返回相同的固定值。相反，我们希望对该方法有一个更好的了解。例如，一种确定的方法是始终获得具有最大时间戳的值。

因此，当供应商说“最终一致性”时，它们的意思是一些更精确的术语，例如“最终最后写入者获胜，并同时读取最新观察到的值”。 “如何写入？” 之所以重要，是因为不良的方法可能导致写入丢失，例如，如果一个节点上的时钟设置不正确并且使用了时间戳。

在有关弱一致性模型的复制方法小节中，我们会更详细地研究这两个问题。

# 3 [Time and order](http://book.mixu.net/distsys/time.html)

什么是顺序（order）呢？为什么我们要关心A是否happens-before B之类的问题呢？为什么我们不关心其他的属性（如color）呢？

回顾下分布式系统的定义，前面我们是这样描述分布式系统的，使用多个解决节点构成的系统来解决原先在单机系统上要解决的相同问题。每个节点在任何时刻都只做一个操作，处理一个事情涉及到的各个节点的一系列操作就会构成一个全局的操作序列。比如人通过一扇门，可能会涉及到开锁、开门、通过、关门、锁门等一系列操作，而这些操作的顺序是很重要的，如果顺序出错，那么结果也会出错。这里操作之间的顺序，就是我们在构建分布式系统过程中要重点关注的。

传统模型是：一个程序，一个进程，一个CPU上运行一个内存空间。操作系统抽象出以下事实：可能存在多个CPU和多个程序，并且计算机上的内存实际上在许多程序之间共享。我并不是说线程编程和面向事件的编程都不存在；只是它们是“程序:进程:内存cpu=1:1:1”模型之上的特殊抽象。程序被编写为以有序的方式执行：程序从顶部开始，然后向下执行到底部。

顺序（order）作为一种属性已经受到了广泛的关注，因为定义“正确性”的最简单方法是说“它就像在单台机器上工作一样”。这通常意味着a）我们运行相同的操作，b）我们以相同的顺序运行它们-即使有多台计算机。

能按照指定的操作顺序（就像在单个系统上执行的那样）执行的分布式系统，它的好处是这样的系统很通用。我们无需担心操作是什么，因为分布式系统将完全像在单台计算机上一样执行。这很棒，因为您知道无论执行什么操作都可以使用该系统来完成。

实际上，一个分布式程序运行在多个节点上，每个节点带有多个CPU和多个操作流。您仍然可以分配一个总的操作顺序，但是它需要准确的时钟或某种形式的通信。您可以使用完全准确的时钟为每个操作加上时间戳，然后使用该时钟来计算总的操作顺序。或者，您可能具有某种通讯系统，可以按总的操作顺序分配顺序号。

## 3.1 全局有序、部分有序

分布式系统中的各项操作，它们之间的顺序是满足部分有序（partial order）的。分布式系统中的网络和独立的节点，二者都不能保证各项操作的相对有序，但是具体到一个特定的节点，节点观察到的本地的操作是有序的（local order）。

当两个不同的元素能够判定其中一个大于另一个时，那它们是可比较的。在部分有序的集合中，某些元素对不具有可比性，因此部分有序不能指定任意两个项目的确切顺序。

全局有序（total order）定义了这样一种关系，从集合中拿出任意两个元素，都可以确定这两个元素的先后关系，这就是全局有序。如果只能拿出部分元素确定这样的可比较关系，那就是部分有序。

全局有序和部分有序，都具备数学上的传递性和非对称性：

- 如果a<=b 并且 b<=a，那么a==b（非对称性）
- 如果a<=b 并且 b<=c，那么a<=c（传递性）

git仓库的不同分支上的提交记录，就是部分有序的。比如从master分支上同一个commit拉出了不同的分支b1、b2、b3，每个分支上有不同的commits，从各个分支来看，其上的commit是有序的，但是横跨多个分支来看，这些commit之间是没有相对顺序的。因此当我们合并多个分支上的commits时是需要基于冲突解决机制来解决，而非按照commit顺序（非全局有序）。

在一个单节点系统中，则肯定是全局有序的，在单个程序中，指令执行、消息处理会按照特定的、可观察到的顺序被处理。我们其实是依赖“全局有序”的，它使得我们能够对程序的执行情况作出合理的预期，或者说程序执行结果是可以预测到的。这里的全局有序在分布式系统中也是可以实现的，但是要付出一定的恒本：

- 通信，将为系统引入额外的通信负载；
- 时钟同步，时钟同步是比较困难的，也比较脆弱；

## 3.2 什么是“时间”？

重新认识一下时间，时间是实现顺序的源泉，它使得我们能够定义各项操作的顺序。对人类而言，它也比较好理解（一秒钟、一分钟、一天，等等）。

从某种意义上讲，时间就像其他整数计数器（counter）一样。因为大多数计算机都具有专用的时间传感器，也称为时钟，所以时间也很重要。至关重要的是，我们已经弄清楚了如何使用一些不完善的物理系统（从蜡制作的蜡烛到铯制作的原子钟）来合成计数器的近似值。通过“合成”，我的意思是我们可以通过某种物理属性在物理上相距较远的地方计算得到近似精确的整数计数器值，而无需跨远距离进行通信。

时间戳实际上是代表从宇宙开始到当前时刻的世界状态的简写值-如果某件事发生在某个特定的时间戳上，那么它可能会受到之前发生的所有事情的影响。 可以将此想法推广为一个因果时钟，该时钟明确地跟踪原因（依赖性），而不是简单地假设时间戳之前的所有内容都是相关的。 当然，通常的假设是我们只应该担心特定系统的状态，而不是整个世界。

假设时间在任何地方都以相同的速度进行-这是一个大的假设，我稍后会再讲-在程序中使用时间和时间戳有几种有用的解释。 三种解释是：

- 顺序（order）：我们说时间是顺序的源头，实际上是说，我们可以通过为发生的无序事件添加时间戳的方式，来对它们进行排序。我们可以借此时间戳对操作或消息传递进行特定的排序，还可以对失序到达的消息进行排队延迟处理。
- 时长（duration）：也会通过测量的时间的长短来和真实世界中任务处理的工作量进行联系。用作时长时，算法通常不关心时钟的绝对值，只需要知道开始、截止时刻之间经过的时间跨度就可以了，如判断一次rpc调用的时长。
- 理解（interpretation）：时间是可比较的，时间戳可以转换为一个日期时间字符串，方便人理解。比如在服务日志中可以把时间戳或者时间字符串写入日志信息中，方便确定服务执行特定动作时的时间信息。

分布式系统中各个节点的操作，节点本地肯定是有一个本地顺序（local order）的，问题不同节点的local order和其他节点的local orders是完全独立的，单看各个节点操作的本地顺序，还是难以对全局操作顺序做出准确的预期、判断。在分布式系统中如果我们不能采取合理的手段构建出这个全局顺序，那我们将很难理解、解释一个结果的可能原因，或者说接下来的处理结果会是什么。本来是一个确定的、唯一的全局操作序列，可能会变为一个错误的排列问题。

## 3.3 各处的时间都以同样的速率前进吗？

根据我们个人的经验，我们都有一个直观的时间概念。不幸的是，这种直观的时间概念，使我们描述全局顺序更加容易，但是描述局部顺序则比较困难。这种直观的时间概念，使我们描述1个接1个发生的事件序列更容易，但是描述并发发生的几个事件或事件序列时则比较困难。对于按固定的顺序到达的消息处理起来比较简单，对于无需到达并且有不同时延的消息处理起来则比较困难。

但是，在实现分布式系统时，我们希望避免对时间和顺序做出强力假设，因为这些假设越强，带有“时间传感器”或“时钟”的系统就越脆弱。此外，维护事件或操作序列的顺序会有一定的成本。系统对临时的不确定性越能容忍，我们就越可以发挥分布式计算的优势。

“各处时间都以同样的速度前进吗？”，这个问题有3个常见的回答：

- 全局时钟（Global clock）：Yes
- 本地时钟（Local clock）：No
- 没有时钟（No clock）：No

这3个回答，与第二章中的三种系统模型是相联系的：

- 同步系统模型（synchronous system model）拥有并依赖全局时钟；
- 部分同步系统模型（partially synchronous system model）拥有并依赖本地时钟；
- 异步系统模型（asynchronous system model）不能使用或依赖时钟；

### 3.3.1 time with a "global clock" assumption

全局时钟假设，指的是假设存在一个完美精度的全局时钟，并且每个人都可以使用该时钟。这就是我们人类更倾向于思考时间的方式，因为在人与人之间的互动中，时间的微小差异影响不大。

![global clock](http://book.mixu.net/distsys/images/global-clock.png)

全局时钟基本上是全局顺序的源头，所有节点上每个操作的确切顺序，这些节点之间并不需要通信来同步时间。

但是，这是一个对时间的过于理想化的认识，实际上，时钟同步是有一定的精度限制的，这与以下因素有关系：商用计算机中时钟的准确性不足；使用时钟同步协议（例如NTP）时的时延；以及时空性质的限制。

假设分布式系统中各个节点上的时钟是完全同步的，即这些节点的时钟以相同的值开始并且永不发生时间漂移。这是一个很好的假设，因为您可以自由地使用时间戳来确定全局顺序（受时钟漂移约束而不是延迟），但做到这点还是比较有挑战性的，并且是一些异常的潜在来源。很多情况下，一个简单的故障都可能会引发很难跟踪的异常，例如用户不小心更改了计算机上的本地时间，或者时间明显落后的节点加入了集群，或者同步过的时钟的漂移速率略有不同，等等。

尽管如此，还是有一些现实世界的系统做出了这种假设，如FAcebook的Cassandra和Google的Spanner就是其中的代表。Facebook的Cassandra使用时间戳来解决写入之间的冲突-带有较新时间戳的写入将获胜。 这意味着如果时钟漂移，新数据可能会被旧数据忽略或覆盖；再次，这是一个运营挑战（据我所知，这是人们敏锐意识到的挑战）。Google的Spanner：该论文描述了其TrueTime API，该API可以同步时间，但也可以估计最坏情况下的时钟漂移。

### 3.3.2 time with a "local clock" assumption

第二个，也许更合理的假设是，每台机器都有自己的时钟，但是没有全局时钟。这意味着您不能使用本地时钟来确定远程时间戳是在本地时间戳之前还是之后发生的；换句话说，您无法比较两台不同计算机上的时间戳，即使您这么比较了，也不能说明什么。

![local clock](http://book.mixu.net/distsys/images/local-clock.png)

本地时钟假设与现实世界更加接近。它能指定了局部顺序：每个系统上的事件都是有序的，但是不能仅通过时钟在整个系统上对事件进行全局排序。

但是，您可以使用时间戳在单台计算机上对本地事件、操作序列进行排序。只要注意不要让时钟跳动（如人为明显调慢或调快时钟），您就可以在一台机器上使用超时。当然，在用户端控制的计算机上，本地时钟的假设，也还是约束太强了。例如，用户在使用操作系统的日期控件查找日期时，可能会不小心将其日期更改为其他值。ps：所以还是需要寻找更靠谱的方式。

### 3.3.3 time with a "no clock" assumption

最后，还有“**逻辑时间**”的概念。这里，我们完全不使用时钟，取而代之的，而是以其他方式来跟踪事件、操作序列中的因果关系。请记住，时间戳只是当时世界状态的一个代指，因此我们可以使用计数器（counter）和通信来确定某件事情是发生在另一件事情之前，还是之后，还是同时。

> 在人对时间的认识中，某个时间点以前的所有事件都先于t完成，可以换种思路，t当前的状态与t之前的所有事件有关，或者与t之前的部分事件有关，这些有关的时间与时刻t的状态存在因果关系。

这样，我们可以确定不同机器上的各个事件的全局顺序，但是由于假定不存在“时钟传感器”，我们完全没有了时间戳的概念，也就不能计算“事件间隔”，也不能使用“超时”机制。如果只借助counter、没有通信，那么我们将只能对本地节点上的事件进行排序，得到的是一个局部顺序（partial order）；如果我们想得到一个全局的顺序（global order），除了counter，我们还需要借助通信来在各个节点之间通知counter的变化。

Lamport在时间、时钟和事件排序方面的论文是分布式系统中引用最多的论文之一。时钟向量，是该概念的概括（我将在后面详细介绍），它是一种无需使用时钟即可跟踪因果关系的方法。Facebook的Cassandra的表兄弟Riak（Basho开发）和Voldemort（Linkedin开发）就使用了时钟向量，而不是假设节点可以访问具有完美准确性的全局时钟。时钟向量，使这些系统可以避免前面提到的时钟精度问题。

当不使用时钟时，采用类似时钟向量的方式，对跨多节点的诸多事件进行排序的最大精度受限于通信时延。

## 3.4 分布式系统中的“时间”是如何应用的？

时间的好处是什么呢？

- 时间可以定义系统内的跨越多节点的顺序（不需要额外的通信）；
- 时间可以用来作为一些算法的边界条件（比如API验签操作）；

分布式系统中的事件顺序是很重要的，因为分布式系统的很多属性都是以操作/事件的顺序来定义的。

- 在分布式系统中，正确性（correctness）是依赖于正确的事件顺序的，例如分布式数据库中的可串行化操作；
- 当发生资源争用时，顺序可以用作平局决胜的参考，例如，如果某个widget有两个顺序，则执行第一个并取消第二个；

如果有一个全局时钟的话，将允许不同机器上的操作可以被排序，并且这个排序过程也不需要不同的机器之间进行直接通信。如果没有这个全局时钟的话，就需要机器之间进行通信，以确定这个顺序。

时间也可以用于作为一些算法的边界条件，比如，为了区分网络通信是否延时高（high latency）或者服务是否挂掉或者通信链路是否故障，就需要利用通信时延来做个判断。这是一个比较重要的案例。大多数真实系统中，超时时间（timeout）被用来判断一个远程主机是否故障，或者判断是否只是经历了一个比较高的网络时延。做出这种决策/判断的算法，被称为失败检测器（failure detectors），后面将会讨论。

### 3.4.1 Lamport时钟（Lamport clocks）

之前，我们讨论了有关分布式系统中时间进度的不同假设。假设我们无法实现精确的时钟同步，也不能假定我们的系统对时钟同步操作相关的问题也不敏感，那么我们怎么对事件/操作进行排序呢？

Lamport时钟和向量时钟，是物理时钟的替代方案，它们依赖于计数器（counter）和通信（communication）来决定跨分布式系统多节点的事件的顺序，这里的时钟维护的计数器是可以跨越多节点进行比较的，这与local clock是不同的。

Lamport时钟比较简单，每个进程维护一个计数器，并遵循如下规则来递增计数器：

- 不管何时一个进程执行一项操作的时候，计数器+1；
- 不管何时一个进程发送一个消息的时候，消息体中带上当前进程中这个计数器的值；
- 不管何时一个进程接收到一个消息的时候，将当前进程的计数器的值设置为max(local_counter,received_counter)+1，即取本地和接收到的远程计数器值中的较大者，并+1；

Lamport时钟维护的计数器，允许跨分布式系统中的节点进行比较，Lamport时钟定义了一个局部顺序，如果 timestamp(a) < timestamp(b)：

- 事件a可能发生在事件b之前，或者；
- 事件a可能和事件b不兼容（计数器不可比较）；

这就是时钟一致性条件（clock consistency condition）：如果一个事件发生在另一个事件之前，那么这个事件的逻辑时钟是要早于另一个事件的。如果事件a和b来自相同的因果历史，比如，两个时间戳的值都是由同一个进程（同一个节点）生成的。或者事件b是一个对事件a中的已发送消息的响应，那么事件a应该发生在事件b之前。

这是因为Lamport时钟只可以携带一个时间线/历史的信息，比较两个从未通信过的系统活节点生成的Lamport时间戳时，可能会让那些无序的并发事件看起来是有序的。

想象一下一个系统，在最初的一段时间后，该系统分为两个彼此不通信的独立子系统。

对于每个独立系统中的所有事件，如果a发生在b之前，则`ts(a) < ts(b)`; 但是如果您从不同的独立系统中获取两个事件（例如，没有因果关系的事件），那么您就无法说出有关其相对顺序的任何有意义的信息。 尽管系统的每个部分都为事件分配了时间戳，但是这些时间戳之间没有任何关系。 即使两个事件不相关，也可能看起来是有序的。

但是这仍然是一个有用的属性，从单台机器的角度来看，任何以ts(a)发送的消息都将收到ts(b)大于ts(a)的响应。

> 思考：Lamport时钟有多巧妙？  
>
> Lamport时钟其实是用了一个比较巧妙的办法，利用计数器和通信，来将不可信的物理时钟（**physical clock**）同步问题转换成了简单的计数器和通信操作这样的逻辑时钟（**logical clock**）。并且将关注的全局时序中所有事件的因果关系问题，转换为思考紧密相关的部分事件的因果关系问题。通过部分事件的因果顺序（casual order）构建出局部顺序（partial order），然后再进一步构建出全部事件的全局顺序（total order）。Lamport时间戳通过维护跨系统多节点可比较的计数器，来定义部分事件的局部顺序（partial order），进一步根据因果关系链再构建出所有事件的因果路径（**casual paths**），并发事件没有casual paths，最终也就得到我们想要的全局顺序（total order）了。  
>
> 更多信息可以阅读：[Logical Time and Lamport Clocks - Part 1](https://medium.com/baseds/logical-time-and-lamport-clocks-part-1-d0317e407112)，[Logical Time and Lamport Clocks - Part 2](https://medium.com/baseds/logical-time-and-lamport-clocks-part-2-272c097dcdda)，也可阅读相关论文。

参考资料：

- [Logical Time and Lamport Clocks - Part 1](https://medium.com/baseds/logical-time-and-lamport-clocks-part-1-d0317e407112) - Vaidehi Joshi, Nov 15, 2019.
- [Logical Time and Lamport Clocks - Part 2](https://medium.com/baseds/logical-time-and-lamport-clocks-part-2-272c097dcdda) - Vaidehi Joshi, Dec 05, 2019.

### 3.4.2 时钟向量（vector clocks, time for causal order）

向量时钟是Lamport时钟的扩展，它维护着由N个逻辑时钟组成的数组[t1，t2，...]，分布式系统中每个节点对应其中一个逻辑时钟。每个节点在每次内部事件时都将向量中其自身的逻辑时钟递增1，而不是递增一个公共计数器。因此，更新规则为：

- 不管何时一个进程执行一项操作的时候，对该进程对应的向量时钟中的逻辑时钟进行递增1的操作；
- 不管何时一个进程发送一个消息的时候，发送的消息中要包含完整的向量时钟信息；
- 不管何时一个进程接收到一个消息的时候：
    - 更新时钟向量中的每一个逻辑时钟，更新规则为max(local, received)；
    - 然后再递增该节点对应的时钟向量中的逻辑时钟；

下图展示了一个向量时钟的示意图：

![vector clock](http://book.mixu.net/distsys/images/vector_clock.svg.png)

三个节点(A,B,C)中的每个节点都跟踪向量时钟。当事件发生时，会用向量时钟的当前值为事件打上一个时间戳。通过检查{A:2，B:4，C:1}这样的向量时钟，我们可以准确地识别（可能）影响该事件的消息。

向量时钟的问题主要是每个节点都需要单独一个条目（逻辑时钟），这意味着它们对于大型系统可能会变得非常大。已经应用了多种技术来减小矢量时钟的大小（通过执行定期垃圾回收或通过限制大小来降低精度）。

我们已经研究了如何在没有物理时钟的情况下跟踪顺序和因果关系。 现在，让我们看看如何将时长用作截止时间。

### 3.4.3 失败检测（failure detectors，time for cutoff）

如前面提到的那样，等待的时间长短可以用来判断一个系统是否出现了分区，或者只是经历了暂时性的通信时延变高的情况。这种情况下，我们不需要一个精确的全局时钟，只要有一个可靠的本地时钟就足够了。

假定一个程序在一个节点上运行，它如何分辨一个远程节点是否发生了故障呢？在没有准确信息的情况下，我们可以推断，一个远程节点如果在约定的合理时间之后没有给出响应，则可以认为该节点出现了故障。

但是，这里的约定的“合理时间”如何确定呢？它取决于通信节点双方之间的网络时延。与其显示地指定具有特定值的算法（在某些情况下不可避免地是错误的），不如做些合理的抽象设计。

故障检测器（failure detector）是一种对精确的时序假设进行抽象的方式。**故障检测器通常基于心跳消息（heartbeat messages）和定时器（timers）来实现**。如果在超时之前没有收到响应消息，那么发送请求的进程将会判定处理请求的进程发生了故障。

基于超时的故障检测器存在如下风险：要么过于激进（声明一个节点发生了故障），要么过于保守（需要花费较长的时间来检测崩溃是否发生）。 故障检测器的精度需要精细到什么程度才能使用？

Chandra等（1996年）在讨论如何解决共识问题时有讨论到了故障检测器。共识问题与故障检测器特别相关，因为它是大多数复制（replication）问题的基础，其中副本（replicas）需要在具有延迟和网络分区的环境中达成一致。

它们使用两个属性（完整性和准确性）来表征故障检测器：

- strong completeness：每个crash的进程最终能被所有正常的进程识别到；
- weak completeness：每个crash的进程最终能被部分进程识别到；
- strong accuracy：没有正常的进程被误判为出现故障或者crash；
- weak accuracy：部分正常的进程被误判，部分没有被误判；

完整性比准确性更容易实现；确实，所有重要的故障检测器都可以实现它-您需要做的就是不要永远等待着怀疑某个人。Chandra等注意到，可以将完整性较弱的故障检测器转换为具有较高完整性的故障检测器（通过广播有关可疑进程的信息），从而使我们可以专注于准确性属性的范围。

避免对非故障进程的误判是非常困难的，除非能够假定最大的消息延迟存在上限值。该假设可以在同步系统模型中进行，因此故障检测器在这种系统中可以非常精确。在不对消息延迟强加任何限制的系统模型下，故障检测最多只能最终准确性（eventually accurate）。

Chandra等表明即使是非常弱的故障检测器 — 最终的弱故障检测器⋄W（最终精度较弱+完整性较弱）也可以用来解决共识问题。 下图（来自本文）说明了系统模型与问题可解决性之间的关系：

![problem solvability in different distributed computing models](http://book.mixu.net/distsys/images/chandra_failure_detectors.png)

TODO take time to understand this diagram!

如您在上面看到的，如果异步系统中缺乏故障检测器，则某些问题是无法解决的。 这是因为，如果没有故障检测器（或关于时间界限的较强约束的假设，例如同步系统模型），就无法判断远程节点是否已崩溃，或者仅仅是经历了高延迟。这种区别对于任何旨在实现单副本一致性的系统都非常重要：可以忽略故障节点，因为它们不会引起分歧，但是不能安全地忽略分区节点。

> 关于单副本一致性的定义（指的其实就是强一致性）：
>
> strict consistency -- "single-copy consistency" -- ensures that an external user can't tell the difference between one machine and several machines. strict consistency and availability fundamentally at odds, esp. in the face of network partitions.

如何实现故障检测器？从概念上讲，简单的故障检测器可能非常简单，如它可以在超时到期时检测故障。故障检测器中最有趣的部分，与如何判断远程节点是否发生故障之类的策略/算法有关。

理想情况下，我们希望故障检测器能够适应不断变化的网络状况，并避免将超时值（timeout）硬编码到其中。例如，Cassandra使用累积故障检测器（accrual failure detector），它是一种输出可疑级别（0到1之间的值）而不是二元判断“up”或“down”的故障检测器。这样，使用故障检测器的应用程序就可以在准确检测与早期检测之间做出权衡取舍。

### 3.4.4 时间、顺序、性能（time，order and performance）

之前有提到对事件/操作进行排序是有开销的，这里的开销到底意味着什么呢？

如果您正在编写分布式系统，则可能拥有一台以上的计算机。世界的本性就是局部有序，而不是全局有序。您可以将局部有序转换为全局有序，但这需要进行通信、等待并添加一些约束，以限制在任何特定时间点可以运行多少台计算机。

所有时钟仅仅是受网络延迟（逻辑时间）或物理限制的近似值。 甚至保持简单的整数计数器在多个节点之间同步也是一个挑战。

虽然通常将时间和顺序一起讨论，但是时间本身并不是一个有用的属性。算法实际上并不关心时间，而是关心更抽象的属性：

- 事件的因果顺序；
- 故障检测（例如，消息传递上限的近似值）；
- 一致的快照（例如，在某个时间点检查系统状态的能力；此处不讨论）；

让事件/操作做到全局有序，实现上是可能的，但是开销比较大。它要求你以普通（最低）速度进行。确保事件以某种定义的顺序传递或执行，最简单的方法就是选出一个节点，然后将所有操作都通过这单个节点来进行，但是这样存在单点故障。

时间、顺序、同步，真的有必要吗？看情况。在某些使用场景下，我们希望每个中间操作将系统从一个一致状态转移到另一个一致状态。例如，在很多场景下，我们想要从数据库的响应中获得所有可用信息，同时我们也希望避免数据库系统返回不一致的结果而引发问题。

但是在其他情况下，我们可能不需要那么多的时间、顺序、同步。例如，如果您运行的是长时间运行的计算，并且直到最后才真正关心系统的工作-那么，只要可以保证答案正确，就不需要太多同步。

当只有一小部分cases对最终结果有实际影响时，同步通常作为一种“迟钝”的手段应用于所有操作。什么时候需要顺序才能保证正确性？CALM定理，我们在最后一章讨论。

TODO what's the meaning of CALM therom?

在其他情况下，给出仅代表最佳估计的答案是可以接受的，也就是说，仅基于系统中包含的全部信息的子集。 特别是，在网络出现分区时，可能只需要访问一部分系统即可回答查询。在其他使用情况下，最终用户无法真正区分可以廉价获得的相对较新的答案和可以保证正确且计算昂贵的答案。例如，某个用户X或X + 1的twitter followers数量是多少？很多情况下，提供更便宜的“尽力而为”的服务是可以接受的。

在接下来的两章中，我们将研究强一致系统中的用于解决容错问题的复制技术（replication），同时对故障的恢复能力越来越强。这些系统为第一种情况提供了解决方案：当您需要保证正确性并愿意为此付出代价时。然后，我们将讨论弱一致性保证的系统，这些系统在面对分区时仍然可以使用，但是只能给您“尽力而为（best effort）”的结果。

# 4 [Replication: preventing divergence](http://book.mixu.net/distsys/replication.html)

复制问题是分布式系统经常要讨论的问题之一，相比leader选举、失败检测、互斥、共识、全局快照问题，我更倾向于介绍复制相关的内容，因为相对来说，复制技术也是大多数人更感兴趣的部分。例如，区分并行数据库的一种方式就是它们采用的复制能力的差异。而且，复制还带为其他一些子问题提供了上下文，如leader选举、失败检测、共识和原子广播。

# 5 [Replication: accepting divergence](http://book.mixu.net/distsys/eventual.html)

# 6 [Appendix](http://book.mixu.net/distsys/appendix.html)
