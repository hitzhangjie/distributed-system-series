# Distributed Systems for Fun and Profit


* [1 [分布式系统高层视角](http://book.mixu.net/distsys/intro.html)](#1-[分布式系统高层视角](http://bookmixunet/distsys/introhtml))
	* [1.1 水平扩展 vs 垂直扩展](#11-水平扩展-vs-垂直扩展)
	* [1.2 分布式系统追求什么](#12-分布式系统追求什么)
	* [1.3 上述目标为何难以实现](#13-上述目标为何难以实现)
	* [1.4 抽象 & 建模](#14-抽象-&-建模)
	* [1.5 设计技巧：“分区”和“复制”](#15-设计技巧：“分区”和“复制”)
* [2 [关注不同抽象层次](http://book.mixu.net/distsys/abstractions.html)](#2-[关注不同抽象层次](http://bookmixunet/distsys/abstractionshtml))
	* [2.1 System Model（系统模型）](#21-system-model（系统模型）)
	* [2.2 系统模型中的节点](#22-系统模型中的节点)
	* [2.3 系统模型中的通信链路](#23-系统模型中的通信链路)
* [2.4 时间/顺序假设](#24-时间/顺序假设)
	* [2.5 共识问题（Consensus Problem）](#25-共识问题（consensus-problem）)
	* [2.6 Two impossibility results](#26-two-impossibility-results)
		* [2.6.1 The FLP impossibility result](#261-the-flp-impossibility-result)
		* [2.6.2 The CAP Theroem](#262-the-cap-theroem)
	* [2.7 一致性模型（consistency model）](#27-一致性模型（consistency-model）)
		* [2.7.1 强一致性模型 VS. 其它一致性模型](#271-强一致性模型-vs-其它一致性模型)
		* [2.7.2 强一致性模型](#272-强一致性模型)
		* [2.7.3 以client为中心的一致性模型](#273-以client为中心的一致性模型)
		* [2.7.4 最终一致性模型](#274-最终一致性模型)
* [3 [Time and order](http://book.mixu.net/distsys/time.html)](#3-[time-and-order](http://bookmixunet/distsys/timehtml))
* [4 [Replication: preventing divergence](http://book.mixu.net/distsys/replication.html)](#4-[replication:-preventing-divergence](http://bookmixunet/distsys/replicationhtml))
* [5 [Replication: accepting divergence](http://book.mixu.net/distsys/eventual.html)](#5-[replication:-accepting-divergence](http://bookmixunet/distsys/eventualhtml))
* [6 [Appendix](http://book.mixu.net/distsys/appendix.html)](#6-[appendix](http://bookmixunet/distsys/appendixhtml))


该系列文章[Distributed Systems for Fun and Profit](http://book.mixu.net/distsys/)主要是介绍了分布式系统编程以及分布式系统的**核心概念**，方便读者后续针对性的深入了解。如果要包含分布式系统的方方面面，那是一件非常疯狂、工作量巨大的事情。

# 1 [分布式系统高层视角](http://book.mixu.net/distsys/intro.html)

## 1.1 水平扩展 vs 垂直扩展

每一个计算机系统都需要关注两方面的事情：**“计算” + “存储”**。
    
假如我们有足够的资金，当计算机硬件能力跟不上时，我们可以考虑升级不停地升级硬件（**垂直扩展，vertical scaling**），甚至是雇佣专门的团队来帮我们设计计算机，就不存在分布式系统的问题了。但我们的资源是有限的，技术发展是有瓶颈的，不可能一直对计算机做升级。

分布式系统，通过构建集群，通过添加新节点的方式来提升整体的处理能力（**水平扩展，horizontal scaling**）。水平扩展才是真正比较可行的解决方案。而且，集群中节点数量增多之后，节点之间的通信也会增多，这里的通信开销也会影响计算效率。所以需要研究高效的分布式算法来优化。

## 1.2 分布式系统追求什么

- Scalability
    
    > is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth.
    
    困难大小源于问题规模。如果要处理的问题规模很小，那困难是微不足道的，比如让“举起”一块巧克力，但是让举起一座山，那就很困难了。当问题规模超过了一定的尺寸、容量或者物理条件限制时，解决问题就会变得非常困难。

    所以说，**everything starts with size - scalability**。在一个设计良好的分布式系统中，系统处理不会因为问题规模从小渐渐变大就变得很糟糕。

    **问题规模增长，指的都是哪些维度的增长**？有3个令人感兴趣的维度供参考。
    - Size scalability：系统集群中添加新节点会线性提升集群计算能力，待处理数据集、请求量增大后并不会明显增加系统处理时延；
    - Geographic scalability： 地理上会使用多地多机房的部署，如在天津、上海、深圳都有部署数据中心，来减少不同地域用户的请求时延。某些时延要求敏感场景下，也需要优化流量调度来减少跨城时延；
    - Administrative scalability：系统集群中添加更多节点不会明显引入额外的管理运维代价； 

    当然在真实的系统中，随着问题规模扩大，系统的“**growth**”是多维度的，不止上面这些。
    
- Performance (and latency)

    > is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used.

     一个可扩展的系统随着规模扩大，依然能够满足用户需求。其中有两个比较重要的指标：performance 和 availability，这两个指标有各种各样的度量方式。关于performance，结合具体上下文，它可能代表了下面几个含义：

     - 针对特定的任务处理，有较短的响应时间（response time）或处理时延（low latency）；
     - 针对特定的任务类型，有较高的吞吐量（high throughput）或较快的处理速率（rate of processing work）；
     - 较低的资源占用率（low utilization of computing resources），可能隐含了更优的算法、逻辑；
     
     针对这些指标对系统做优化时，涉及到一系列的权衡、折中（tradeoffs）。比如，系统可以通过处理较大型的作业来提升系统吞吐量，作业较小可能意味着人为的干预、会中断系统处理，但是较大型的作业的处理结束的响应时间也会变长，那这里在响应时间和吞吐量之间就做了折中。

     处理时延latency的含义，与响应时间差不太多：

     > The state of being latent; delay, a period between the initiation of something and the occurrence.

     在分布式系统中，latency不可能为0，如信息需要通过电信号传输，速度可以认为是光速，看上去很快，latency可以忽略不计？No，硬件有自己的物理特性，硬件操作都是有时延的，尽管看起来很小，但是这些时延会叠加起来……所以latency不可能为0或忽略不计。最终latency的大小，取决于任务处理的工作量大小以及信息传输的距离。
   

- Availability (and fault tolerance)

    > the proportion of time a system is in a functioning condition. If a user cannot access the system, it is said to be unavailable.

    分布式系统使得我们可以获得单机系统没有的特性，如容错。单机部署的服务如果挂掉了，是没法容错的，只能事后重启来应对。
    
    分布式系统中，其实应用了很多不可靠的组件，但是将这些组件结合起来却可以构建一个可靠的分布式系统。系统通过冗余（redundancy）可以实现部分容错，可以获得更好的可用性（availability）。

    冗余（redundancy），结合上下文，也可以指代不同的东西，如组件的冗余、server的冗余、数据中心的冗余、网络的冗余，等等。

    可用性的计算公式，很简单，`availability = uptime / (uptime + downtime)`，业界通常用“几个9”来表示可用性级别，通过不同可用性级别对应的每年停机时间，会获得一个非常直观的感受：

    |可用性级别     |每年停机/故障时间|
    |---------------|-----------------|
    |90%，1个9      |超过1个月        |
    |99%，2个9      |不足4天          |
    |99.9%，3个9    |不超过9个小时    |
    |99.99%，4个9   |不超过1个小时    |
    |99.999%，5个9  |不超过5分钟      |
    |99.9999%，6个9 |不超过31秒       |

    可用性的概念，不能仅仅停留于系统上线时间（uptime），更精准地描述，应该是系统能否正常工作、满足用户请求。这样的话要求就更高了，因为涉及到了响应时间等具体的硬指标。

    一个系统的可用性要达标，需要考虑方方面面的事情，系统走的网络、数据中心、应用的组件、本身部署的机房、依赖的外部API，等等，因为不能假定这些things都是可靠、不可能出问题的，所以在设计的时候最好就要考虑到冗余设计，来完成部分容错，不然不可能获得一个高可用的分布式系统。

    那么，容错（fault tolerance）指的是什么呢：

    > ability of a system to behave in a well-defined manner once faults occur

    要实现容错，前提是要知道错误或者故障的类型，然后才能设计对应的系统或者提出一种算法来在遇到该错误时进行处理、补偿。不可能对未考虑过的错误进行自动容错。

## 1.3 上述目标为何难以实现

分布式系统，受限于两个物理上的因素：

- 系统中节点的数量（节点数量增加，提高了系统的存储容量和计算能力）；
- 节点之间的物理距离（信息传输需要时间，最好情况下是光速传输，但是硬件自己的电器特性还是有开销的）；

在这些约束基础上，我们再来看下：

- 系统中节点的数量增加，也意味着节点出现故障的概率更大了（任一节点正常工作概率为p (p<0)，则系统中出现故障节点概率为1-p^n），会降低可用性、增加运维成本；
- 系统中节点的数量增加，也增加了节点之间的通信开销（节点要与其他节点通信，通信请求数量多了），随着集群规模扩大，通信引入的开销会降低节点的计算性能；
- 系统中节点的数量增加，如果节点间地理位置增大了，节点间最小通信时延会增加，会增加系统处理或特定操作的响应时间；

    ![cluster size (number of cores)](http://book.mixu.net/distsys/images/barroso_holzle.png)

分布式系统设计，需要先确定设计目标，设计实现时需要为了设计目标来做各种现实的权衡、折中。通常这里的设计目标，需要以各种保证条款的形式沉淀下来，如SLA（Service Level Agreement）。如，如果一次数据写操作成功，隔多久可以从其他地方访问最新数据？或者数据写入之后，数据持久性能保持多久？或者针对一次计算，隔多久可以计算完成返回结果？再或者说，如果一个组件失败，或者某个操作出问题，对系统会造成多大范围的影响、事故？

系统是代替人干活的，所以还有另一个层面的问题，系统的可理解性（intelligibility）。系统处理的结果、给出的信息、告警信息、错误信息等等是不是易于理解？有多么容易让人理解？

我们见过设计的很糟糕的系统，出点问题，经常需要拉起很多人、用很多时间、经过多番讨论、监控数据对比才能定位到是哪个server发生了问题、问题又是如何传播引起了整个系统的故障。对可理解性方面，没有明确的一些metrics可供度量、列出，但是它依旧是一个非常重要的维度。还有就是，一个系统的运行状态是否正常，是否能够做出判断，是否一定需要有经验的人才能做出诊断。这也是一些可以使用的metrics。

## 1.4 抽象 & 建模

前面提到的这些问题，就需要通过一定的抽象和建模来解决。

- 抽象（abstractions），指的是对要解决的一些具体的问题，做一些褪去无关细节、提升思考维度做些通用性设计，以解决一类问题的方法；
- 建模（models），以更精确的方式来描述了一个分布式系统应该具备的各种关键属性；

    - System model (asynchronous / synchronous)
    - Failure model (crash-fail, partitions, Byzantine)
    - Consistency model (strong, eventual)

    ps: 关于建模的部分，请见本文下面的描述。

良好的抽象使系统更容易理解，同时能够捕获与特定问题相关的各种影响因素。

分布式系统中包含了很多节点，我们希望分布式系统能像一个单机系统那样易于理解，在这之间就存在一点冲突。比如，我们希望在一个分布式系统上实现一个共享内存的抽象设计，共享内存虽好理解，但是实现基于分布式系统实现却很复杂。这样看来，常常我们单机系统中熟悉的模型，在分布式系统中实现起来会很复杂。

一个系统，如果对各种保证方面有更弱的保证，通常会获得更大的自由度，也会因此而获得更好的系统性能，但另一方面，也会变得更难于理解。我们很容易理解一个单机系统中发生的事情，但是对于包含很多节点的分布式系统，要说清楚其中发生的事件则相对困难，尤其是一些异常、错误。

人们通常可以通过公开有关系统内部的更多细节来获得性能。 例如，在列式存储中，用户可以（在某种程度上）推断键值对在系统中的位置，从而做出影响典型查询性能的决策。 隐藏这些细节的系统更易于理解（因为它们的行为更像是一个单元，更少的细节需要考虑），而暴露更多真实细节的系统则可能更具性能（因为它们与现实的关系更紧密） 。

个别类型的故障使实现像单机系统一样工作的分布式系统变得非常困难。网络时延（network latency）和网络分区（network partition，total network failures between some nodes）意味着当系统中出现这类故障时，系统有时需要做出艰难的选择，以便更好地保持可用性，但失去了一些无法强制执行的关键保证，或者为了其安全性就拒绝了客户端请求。

下文中会提及CAP定理，来进一步解释这里遇到的一些问题。最后，理想的系统既可以满足程序员的需求（清晰的语义）又可以满足业务的需求（可用性、一致性、延迟等）。

## 1.5 设计技巧：“分区”和“复制”

分布式系统中有很多节点，节点要做计算就需要先知道待计算的数据是如何存储的、存储在哪里，数据集是如何存储的对分布式系统而言是非常重要的。

对于数据集存储，通常有两种方式：

- 分区（partition），数据集可以根据一定的规则做适当的分割（split）然后存储到不同的节点上；

    > is dividing the dataset into smaller distinct independent sets; this is used to reduce the impact of dataset growth since each partition is a subset of the data.

    分区把数据集按照特定split规则进行分割，如按照用户uid将不同用户信息切分开，然后存储到不同的节点上。相比完整数据集来讲，单一分区上的数据量大幅减小，有助于实现数据的最终存储，以及后续数据的高效查询。

    即便后续有的节点出现了失败，也只会影响该节点上存储的分区，并不会影响到整个的数据集，其他节点上存储的分区依然是可用的。并且这里出问题的节点，我们也可以进一步通过复制来增强可用性。    
    分区，和具体的数据集分割的规则有关系，和应用场景有关系的，这里就先不多说了，我们将重点关注“复制”。
     
- 复制（replication），数据集也可以被复制（copied）或缓存（cached）到不同的节点上，以减少为了访问其他节点存储的数据引入的通信时延，也可以实现更好的容错，避免节点故障引入的数据丢失；
    
    > copying or reproducing something - is the primary way in which we can fight latency.

    复制，为数据生成了额外的副本，提高了容错能力，提高了系统的可用性。存储这些数据副本的节点也可以用来对外提供服务，也提升了系统的整体性能、降低处理时延（latency）。复制，提供了额外的带宽（bandwidth），复制在延时敏感领域可以通过缓存降低数据访问时延。复制，和维护数据一致性也息息相关（如WAL，Write Ahead Log）。

    复制，允许我们获得更好的扩展性、性能、容错能力，有了复制，就不用那么担心可用性受损或者数据访问性能下降的问题，可以通过复制数据到其他节点来降低系统访问瓶颈和数据单点故障。复制的概念可以进一步延伸，如复制计算，担心计算性能问题，将计算复制到多个系统，采用分布式计算的方式。担心慢速IO，将数据复制到本地cache来降低延时，或者复制到多机来提升数据访问的吞吐量。

    另外，也要意识到，复制也是造成一些难解问题的原因，比如数据一致性问题。数据复制到多个节点，如何维护这些数据副本的一致性，在分布式领域是一个非常重要的问题，**数据一致性模型**的选择对维护数据一致性至关重要。好的一致性模型即具有清晰的语义（如方便理解）也能满足业务要求（如数据强一致、高可用等）。
    
    对于复制，只有强一致模型才能够像对待没有数据复制一样轻松地编码实现。其他一致性模型，都或多或少暴露了一些数据复制的细节信息给开发人员，开发人员需要理解这样的复制存在的问题，如数据不总是一致的，需要借助一些其他手段来识别数据是否ok，编码上就没那么简单。但是，弱一致性模型，因为约束少、自由度大，往往可以获得更好的系统性能、更高的可用性，当然也并不是那么难以理解。
    
    业界存在一些经过验证的一致性算法如Paxos、Raft、ZAB等等，这些一致性算法既需要考虑如何维护数据一致性，也需要考虑降低维护一致性的开销、保证分布式系统的处理性能。

下图中展示了分区、复制二者之间的一个区别，图中数据集A和B是以分区的形式存储到了不同节点Node1和Node2上，数据集C则是以复制的方式在Node1、Node2上进行了存储。

![parition and replicated](http://book.mixu.net/distsys/images/part-repl.png)

不同的分布式系统可能会选择不同的数据存储策略，可能是分区，也可能是复制，更常见的是两者的集合，数据集会分区存储到不同节点，同时也会在其他节点生成数据的副本。现在需要了解的是，分区、复制这两种方式，它们各有优缺点，分区、复制也有不同的算法来支持，需要结合具体的设计目标来选择和评估。


参考资料：
- [The Datacenter as a Computer - An Introduction to the Design of Warehouse-Scale Machines](http://www.morganclaypool.com/doi/pdf/10.2200/s00193ed1v01y200905cac006) - Barroso & Hölzle, 2008
- [Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing)
- [Notes on Distributed Systems for Young Bloods](http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/) - Hodges, 2013

# 2 [关注不同抽象层次](http://book.mixu.net/distsys/abstractions.html)

我们先在不同抽象层次上观察，来看一些“**不可能的结果**”（impossibility results，如CAP和FLP），然后再把注意力转向更具体的层次来关注下“性能”。

经常编程的话，应该熟悉“抽象层次”的提法。开发人员通常工作在某个抽象层次上，并且通过API调用与更下层进行交互，并且当前抽象层次也需要提供API给更上层的应用调用或者与用户交互。七层OSI网络模型便是一个很好的例子。

分布式编程很大程度上是在处理“分布"的问题。我们的系统中有很多的节点，但我们希望它能像一个单机系统一样工作，这两者之间是存在矛盾的。意味着，我们需要找到一个好的抽象，以在系统的可能性、可理解性和性能之间达到一个平衡。

抽象层次中，当我们说X比Y更抽象时，是什么意思呢？首先，X不会引入任何新的或与Y根本不同的东西。实际上，X可能会删除Y才有的某些东西或从更易于管理的角度出发来进行一般化设计。其次，X移走的某些Y的东西对于手头的事情并不重要，X比Y更容易掌握。抽象，更简单的问题描述，褪去了各种无关的具体细节、又保留了本质，更容易被分析，提出的解决方案也更具有普适性。

实际上，如果我们能够保留一些问题的本质，那么结果、派生结果也会具有相对广泛的应用。这就是不可能结果问题（impossiblility results）非常重要的原因：它们对问题进行了简化，基本上是用最简单的问题形式，来揭示即便是施加了一些约束、限制、假设后，我们仍然不可能解决该问题。从而针对该问题的更一般化的问题，可以得出我们更无法解决的结论。

所有的抽象都忽略了一些东西，抽象的诀窍就是扔掉所有不必要的东西。你怎么知道什么是最重要的呢？嗯，你可能不会事先知道。

每次我们从系统规范中排除系统的某些方面时，都会冒引入错误和性能问题的风险。这就是为什么有时我们需要朝另一个方向前进，并有选择地引入真实硬件和真实世界问题的某些方面。重新引入某些特定的硬件特性（例如，物理顺序）或其他物理特性，可能足以使系统运行良好。

对于分布式系统时，抽象设计时，需要保留的最少现实信息的度如何把握呢？系统模型（system model），是这方面的一个重要规范。指定了一个系统模型之后，我们可以进一步看看相关的不可能结果和面临的挑战。

## 2.1 System Model（系统模型）

> a set of assumptions about the environment and facilities on which a distributed system is implemented.

分布式系统的一个关键属性就是“分布式”，分布式系统中的程序：

- 在众多相互独立的节点上并发运行；
- 众多的节点（或程序）之间通过网络进行互联、通信，会引入一定的不确定性和消息丢失；
- ...
- 没有共享内存；
- 没有共享时钟；

上面这些问题，暗示着：

- 分布式系统中的每个节点以并发的形式运行程序；
- 分布式系统中的每个节点掌握的信息都是偏向于本地（或局部）的：节点可以快速节点本地的状态信息，但是访问全局状态信息则很可能获取到的是过时的信息；
- 分布式系统中的每个节点都可能出现失败，也可能从失败中恢复，这些情况都是可能相互独立出现的；
- 分布式系统中的消息可能出现延迟送达或者丢失的情况（节点失败具有独立性，区分网络失败或者节点失败并不是一件很简单的事情）；
- 分布式系统中各个节点的时钟可能出现不同步（一个节点的本地时间戳，不一定总能代表全局的时间戳，多个节点执行操作顺序，用各个节点的本地时间戳来排序从系统全局的时间戳来看可能是排序错误的）；

一个系统模型（system model），需要列举与特定系统设计相关的许多假设。不同的系统模型在关于环境（environment）和设施（facilities）的假设方面会有所不同。这些假设包括：

- 节点要实现哪些功能，以及会如何失败；
- 节点之间的通信链路是怎样的，以及会如何失败；
- 系统整体的一些属性，如关于时间和顺序（time and order）的假设；

一个健壮的系统模型，提出的假设在约束性方面应该尽可能弱：这样设计出来的算法也更健壮，能够容忍系统在各种不同的环境中运行，因为它做少了更少、更弱的假设。

另一方面，我们可以通过做出强有力的假设来创建易于推理的系统模型。 例如，假设节点没有故障意味着我们的算法不需要处理节点故障。 但是，这样的系统模型是不现实的，因此难以应用于实践。

让我们更详细地看一下节点的属性、通信链路以及时间和顺序。

## 2.2 系统模型中的节点

系统中的节点，充当计算、存储用的主机，节点通常具备：

- 执行程序的能力；
- 存储数据到易失性存储器、持久化存储器（节点失败后也可以从中读取数据）的能力；
- 拥有一个时钟（可能不准确，并且要假定它可能不准确）；

节点执行的是确定性的算法：执行本地计算，存储计算后的本地状态，发送出去的消息由接收到的消息以及当时的本地状态唯一确定。

有许多可能的故障模型（failure models）描述了节点可能出现故障的方式。 实际上，大多数系统都采用崩溃恢复故障模型（crash-recovery failure model）：也就是说，节点只能通过崩溃来发生故障，并且（可能）可以在崩溃以后的某个时刻恢复。

另一种选择是假设节点可以通过任意的错误行为而出现故障、失败，这种称为拜占庭故障（Byzantine faults）。拜占庭故障在现实世界的商业系统中是很少处理的，因为能够抵抗任意故障的算法运行起来会很复杂、代价高、实现也复杂。我们这里不会讨论这个，可以了解下Paxos或者Raft一致性算法。

## 2.3 系统模型中的通信链路

通信链路将一个节点和其他节点连接在一起，并且允许在连接的节点之间互相传递消息。很多讨论分布式算法的书中，假定系统中的任意两个节点之间都是有独立的链路连接的，并且提供FIFO有序的消息传递，这里发送的消息可能会出现丢失的情况。

有些算法假定网络是可靠的：链路上传递上的消息永远不会出现丢失、延迟送达。在真实世界中的某些场景下，这种假设可能是成立的，但是对绝大部分场景而言，还是假设网络不可靠比较好，尤其要考虑网络中的消息丢失和延迟的影响。

如果系统中出现了网络失败，就可能会形成网络分区，分区中的节点还是可以保持工作，只是不同分区中的节点不能相互通信了。当网络故障后出现分区时，分区之间的消息传递可能会出现丢失、延迟，直到网络分区被修复。不同分区中的节点仍然可以被某些clients访问，分区中的节点也可以提供服务，这与节点crash后无法提供服务是两种明显不同的情况，必须要对这两种情况进行区分对待。下图显示了节点失败和网络分区的故障发生点的不同，前者是节点，后者是网络：

![node failure vs. network partition](http://book.mixu.net/distsys/images/system-of-2.png)

很少会对通信链路做一些假设，我们可以假设通信链路是单向传输的，或者我们可以引入对不同通信链路的通信开销的考量（如不同物理距离的通信时延）。但是，在真实的商业系统中需要做这种假设的情况比较少，除非涉及到较远距离的通信（如广域网WAN通信），所以这里也不会做过多假设。成本和拓扑的更详细模型可以以复杂性为代价进行更好的优化。

# 2.4 时间/顺序假设

分布式系统中节点的不同物理分布带来的一个问题是，每个节点对这个世界的“感受”都是不同的。如果节点彼此之间的距离不同，则从一个节点发送到另一个节点的信息将在不同的时间到达，并且可能以不同的顺序到达其他节点。

时间假设在不同的系统模型里面也是不同的，我们主要关注的两个模型：

- 同步系统模型（synchronous system model）

    > Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock.

    同步系统模型对时间和顺序施加了许多约束。它基本上假设节点具有相同的体验：发送的消息始终能在最大传输延迟内被接收，并且过程以lockstep方式执行。这很方便，因为它使您可以对时间和顺序进行假设，而异步系统模型则不能。

- 异步系统模型（asynchronous system model）

    > No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist.

    异步模型中，您不能依赖定时（或“时间传感器”）。

    解决同步系统模型中的问题更容易，因为相关的假设（如执行速度、最大消息传输延迟和时钟准确性）都有助于解决问题，因为您可以基于这些假设进行推断来排除一些故障。

    当然，同步系统模型在现实中并非总是成立。现实世界的网络容易出现故障，并且对消息延迟没有严格的限制。现实世界的系统充其量只能是部分同步模型：它们有时可能会正常工作并提供一些消息传输时延、始终准确性的上限，但是有时消息会无限期地延迟并且时钟不同步。我不会在这里讨论同步系统的算法，但是您可能会在许多其他入门书中碰到它们，因为它们在分析上比较容易。

参考资料：

- [lockstep (computing)](https://en.wikipedia.org/wiki/Lockstep_(computing))

## 2.5 共识问题（Consensus Problem）

接下来我们将更改系统模型的参数，再接下来，我们将研究改变两个系统属性（**网络分区、同步或异步时序**）之后将如何影响系统设计。

- 系统模型中是否考虑了网络分区问题；
- 系统模型中是否考虑了同步、异步时序问题；

讨论上述问题如何影响系统设计，需要考虑两个**不可能的结果（impossibility results），FLP以及CAP**。

当然，为了展开讨论，我们需要引入一个问题，在尝试解决的过程中展开讨论，这里引入的问题是共识问题（consensus problem）。

当多台计算机（或者多个节点）能够在一些“值”上面达成一致，就说它们达成了共识。

“**共识问题**”更正式地描述，可以归纳为：

Agreement：每个正确的进程，都必须商定相同的“值”；
Integrity：每个正确的进程，最多能决定一个“值”，如果系统最终商定了一个值，这个值一定是由某个（或某些）进程确定的；
Termination：所有进程最终都必须能做出决定；
Validity：如果所有正确的进程都提出了相同的值V，那么所有正确的进程都将决定值使用值V在值V；

共识问题，是很多商用分布式系统的核心。毕竟，我们希望系统能够具备分布式系统的可靠性和性能的同时，能够不用去处理琐碎的“分布式”的问题（例如，节点之间的分歧），解决共识问题可以解决一些相关的更高级的问题，例如原子广播（atomic broadcast）和原子提交（atomic commit）。

## 2.6 Two impossibility results

第一个不可变结果问题，是FLP不可变结果（FLP impossibility result），该不可变结果用来指导分布式算法的设计人员。

    >FLP不可变结果：在一个完全异步的环境中，不可能找到有效的共识算法。

第二个不可变结果问题，是CAP理论，该不可变结果更倾向于出现网络分区时对系统设计人员的一些指导，而不是对分布式算法人员的直接指导。

    >CAP定理：在一致性（Consistency）、可用性（Availability）、分区耐受性（Partition Tolerance）三者中，分布式系统只能三选二。

### 2.6.1 The FLP impossibility result

FLP不可能结果，在分布式相关的学术研究中FLP不可能及其证明具有非常重要的地位，感兴趣的可以参考 /materials/A Brief Tour of FLP Impossibility.pdf，本文这里只对FLP不可能结果进行概述。FLP不可能结果是以3名研究人员姓名命名的（分别是Fischer、Lynch和Patterson）。FLP不可能结果，研究了异步模型下的共识问题（技术上是agreement问题，是共识问题的一种非常弱的形式）。FLP不可能结果证明论文中，做了这样的假设：假定节点只能因崩溃而失败；假定网络是可靠的，并且异步系统模型的典型时序假设成立，即对消息延迟没有任何的约束、限制。

在这些假设下，FLP不可能结果表明：“在一个遭受故障的异步系统中，不存在一个确定性的算法能够用来解决共识问题，即使系统中永远不丢失消息、最多一个进程失败（且因为崩溃失败，崩溃就是停止执行）”。

这个结果意味着，即使在一个非常小的系统模型中，不可能以一种永远没有延迟的方式解决共识问题。论据是，如果存在这样的算法，则可以设计该算法的执行，其中该算法将通过延迟消息传递而在任意时刻保持不确定状态（“二价”，bivalent），这在异步系统模型中是允许的。这与假设冲突，因此这种算法不存在。详细的FLP不可能结果证明，可以参考 /materials/A Brief Tour FLP Impossibility.pdf。

**FLP不可能结果很重要，因为它强调了对异步系统模型不得不做这样一个折衷：当消息传递没有保证时，解决共识问题的算法必须放弃安全性（safety）或者放弃活性（liveness）**。

那么，如何理解safety和liveness呢？

首先，一个并发程序的执行过程可以看做是一个无穷的状态序列，下面是safety和liveness的非正式定义：

- safety：safety属性指的是，一些'bad thing'在程序执行期间不发生。safety属性相关的示例有互斥、消除死锁、部分成功、先来先服务；
- liveness：liveness属性指的是，一些'good thing'在程序执行期间能发生。liveness属性相关的示例包括避免饿死、算法正常终止、确保服务；

其他的属性，其实可以说是safety（避免bad thing发生）、liveness（让good thing发生）的一些取交操作衍生而来。


FLP不可能问题，或者说结论，与算法设计人员特别相关，因为它对异步模型中我们可以解决的问题添加了严格的约束。

ps：消息传递在异步系统中，几乎总是不可能保证的，所以解决共识问题的算法只能在安全性或可用性之间做取舍。我认为是FLP不可能结果基础上，进一步催生了CAP理论。CAP定理是与从业人员更相关的一个相关定理：它做了与FLP稍微不同的假设（假设存在网络故障而不是节点故障），并且对从业人员在系统设计中进行取舍具有更明显的含义。

参考资料：
- [A Brief Tour of FLP Impossibility.pdf](https://www.the-paper-trail.org/post/2008-08-13-a-brief-tour-of-flp-impossibility)
- [Impossibility of distributed consensus with one faulty process](http://scholar.google.com/scholar?q=Impossibility+of+distributed+consensus+with+one+faulty+process) - Fischer, Lynch and Patterson, 1985
- [Defining liveness](https://www.sciencedirect.com/science/article/abs/pii/0020019085900560?via%3Dihub) - Bowen Alpern, Fred B. Schneider


### 2.6.2 The CAP Theroem

    > a **deterministic** consensus protocol cannot have **liveness**, **safety** and **fault tolerance** in a fully asynchronous system.

CAP定理最初是由计算机科学家Eric Brewer提出的一个猜想。 在系统设计的保证中考虑折衷是一种流行且相当有用的方法。它甚至有Gilbert和Lynch的正式证明。

该定理说明了这三个属性：

- 一致性：所有节点同时看到相同的数据；
- 可用性：节点故障不会阻止幸存者继续运行；
- 分区耐受性：网络和节点故障会导致消息丢失，这种情况下系统仍可继续运行；

CAP理论表明，我们同时只能满足两个条件。我们甚至可以将其绘制成漂亮的图，从三个属性中选择两个属性，可以得到对应于不同交集的三种类型的系统：

![CAP三选二可得到的3种系统](http://book.mixu.net/distsys/images/CAP.png)

请注意，该定理指出中间区域对应的系统（具有所有三个属性）是无法实现的。 然后我们得到三种不同的系统类型：

- CA（consistency + availability），相关的实例如一些很严格的协议，例如两阶段提交；
- CP（consistency + partition tolerance），相关的示例包括多数协议（协议中假定少数分区不可用），例如Paxos；
- AP（availability + partition tolerance），相关的示例如一些依赖冲突解决的协议，如dynamo；

CA和CP系统设计均提供相同的一致性模型：高度一致性。 唯一的区别是CA系统不能容忍任何节点故障。 在非拜占庭式故障模型中，CP系统最多可以容忍给定2f + 1个节点的f个故障（换句话说，只要多数f + 1处于故障状态，CP系统就可以容忍少数f个节点的故障）。 原因很简单：

- CA系统无法区分节点故障和网络故障，因此必须停止在所有地方接受写入，以避免引入差异（多个副本）。 它无法确定远程节点是否已关闭，或者仅网络连接是否已关闭：因此唯一安全的事情是停止接受写入。
- CP系统通过在分区的两侧强制非对称行为来防止不一致（例如，保持单副本一致性）。 它仅保留多数分区，并要求少数分区不可用（例如，停止接受写入），这保留了一定程度的可用性（多数分区），并且仍确保单副本一致性。

当我讨论Paxos时，我将在复制一章中对此进行更详细的讨论。 重要的是，CP系统将网络分区纳入到其故障模型中，并使用Paxos、Raft或viewstamped一致性算法来区分多数分区和少数分区。CA系统不支持分区，并且在历史上更为常见：它们通常使用两阶段提交算法，并且在传统的分布式关系数据库中很常见。

假设发生分区，则该定理简化为可用性和一致性之间的二元选择。

![CAP如果分区必然发生](http://book.mixu.net/distsys/images/CAP_choice.png)

我认为应该从CAP定理中得出四个结论：

- 首先，早期分布式关系数据库系统中使用的许多系统设计都没有考虑分区耐受性（例如，它们是CA设计）；

  分区耐受性是现代系统的一项很重要的属性，因为网络分区是很可能出现的，对于一些多地部署的大型系统更是这样。

- 其次，在网络分区过程中，强一致性和高可用性之间存在冲突关系；

  CAP理论概述了在强一致和分布式计算之间进行取舍的情况。某种意义上，保证一个分布式系统与非分布式系统没有任何区别是非常疯狂的，因为分布式系统往往是由不可靠网络连接很多独立节点而构成的，有很多问题需要尝试去解决。

  强一致性保证，要求在出现网络分区时放弃可用性，为什么呢，因为在当两个副本（replicas）之间不能正常通信时，这些副本可能还在接收不同的写请求，这样的话，我们是不能阻止它们出现数据不一致的。
  如何解决这个问题呢？可以对假设进行加强（假定不会出现分区）或者弱化这里的一致性保证。可以拿一致性和可用性（在线服务、低延时……）做个权衡、折中。通常强一致性可以定义成：“所有的节点在同一时刻看到的同一个数据的值是相同的”，如果系统中对一致性的定义要比这个弱一些的话，那我们就可以在可用性和（相对弱的）一致性之间做点折中。

- 第三，在正常操作中，强一致性和性能之间存在冲突；

  强一致性/单副本一致性，要求节点进行通信并就每个操作达成一致，这会导致正常操作期间的高延迟。如果您可以使用经典模型以外的一致性模型（允许副本滞后或暂时不一致）的一致性模型，则可以减少正常操作期间的延迟并在存在分区的情况下保持可用性。

  当涉及较少的消息和较少的节点时，操作可以更快地完成。但是实现此目标的唯一方法是放宽保证：让某些节点的联系频率降低，这意味着节点可以包含旧数据。

  这也使得可能发生异常。您不再保证获得最新的值。根据做出何种保证，您可能读取的值比预期的要旧，甚至丢失一些更新。

- 第四，间接得出，如果我们不想在网络分区期间放弃可用性，那么我们需要研究强一致性以外的一致性模型是否对我们的目的可行。

  例如，即使将用户数据地理复制到多个数据中心，并且这两个数据中心之间的网络连接暂时出现故障，在许多情况下，我们仍然希望允许用户使用网站/服务。这意味着以后需要协调两组不同的数据，这既是技术挑战，又是业务风险。但是技术挑战和业务风险通常都是可控的，因此最好提供高可用性。

  一致性和可用性并不是真正的二选一问题，除非要实现强一致性。但是强一致性只是一致性模型中的一种：在这种模型中必须放弃可用性模型，以防止激活多个数据副本。

  如果您仅从本次讨论中删除一个想法，那就这样吧：“一致性”不是一个孤立的、明确的属性。 记住：[ACID](https://en.wikipedia.org/wiki/ACID) consistency != [CAP](https://en.wikipedia.org/wiki/CAP_theorem) consistency != [Oatmeal](https://en.wikipedia.org/wiki/Oatmeal) consistency。
  
  相反，**一致性模型是数据存储给使用它的程序的保证**-任何保证。

## 2.7 一致性模型（consistency model）

  > consistency model: a contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictable.

CAP中的“C”指的是“强一致性”，但是“consistency”并不是“强一致性”的同义词，需要明白这点。

### 2.7.1 强一致性模型 VS. 其它一致性模型

一致性模型可以分为两类：**强一致性模型** 和 **弱一致性模型**：

- 强一致性模型（strong consistency models）维护一个副本
    - linearizable consistency
    - sequential consistency
- 弱一致性模型（weak consistency models）
    - client-centric consistency models
    - causal consistency: strongest model available
    - eventual consistency models

强一致性模型，保证更新的明显顺序和可见性，与一个非复制的系统完全一致。弱一致性模型，则不做这种保证。

请注意，这绝不是详尽的一致性模型的清单。一致性模型只是程序员和系统之间的契约，因此它们几乎可以是任何东西。

### 2.7.2 强一致性模型

强一致性模型可以进一步划分为两类，它们有点相似但又有一点不同：

- 线性一致性（linearizable consistency）：在线性化一致性下，所有操作似乎都按照与操作的全局实时顺序一致的顺序自动执行。 （Herlihy＆Wing，1991）
- 顺序一致性（sequential consistency）：在顺序一致性下，所有操作似乎都以某种顺序原子执行，该顺序与单个节点上看到的顺序一致，并且在所有节点上都相等。 （Lamport，1979年）

关键区别在于，线性化一致性要求操作生效的顺序等于操作的实际实时顺序。顺序一致性允许对操作进行重新排序，只要在每个节点上观察到的顺序保持一致即可。有人可以区分两者的唯一方法是，他们可以观察进入系统的所有输入和时序。从客户端与节点交互的角度来看，两者是等效的。

差异似乎无关紧要，但是值得注意的是顺序一致性并没有组成（doesn't compose）。

强一致性模型允许开发人员将访问一个单节点服务轻松替换为访问一个分布式多节点构成的集群，而不会引发任何问题。

所有其他一致性模型都会引发异常（与保证强一致性的系统相比），因为它们的行为方式与非复制系统不同。但是这些异常通常是可以接受的，或者是因为我们不关心偶发性问题，或者是因为我们编写了处理不一致问题的代码。

请注意，对于弱一致性模型，实际上并没有任何通用类型，因为“不是强一致性模型”（例如“以某种方式可与非复制系统区分开”）几乎可以是任何东西。

### 2.7.3 以client为中心的一致性模型

以client为中心的一致性模型，指的是这样的一致性模型：它以某种方式围绕客户端或者会话（session）展开。例如，系统会保证客户端不会看到比当前看到的数据更旧版本的数据。通常可以在client library中增设cache，如果访问后端返回的数据版本比当前缓存的数据更旧，那么直接返回缓存的数据即可，而不是副本中更旧的数据。

这种情况下，client如果初始连接到了一个存有旧数据的副本，获取并缓存了旧版本的数据，这种情况下client也会看到旧数据，但是当它后续访问时，不会访问到比这个版本更旧的数据。如果缓存过期，或者异步更新缓存时发现了副本中更新版本的数据，则可以更新client缓存的数据。

有很多类型的一致性模型，属于以client为中心的一致性模型。

### 2.7.4 最终一致性模型

最终一致性模型，意思是说，如果我们停止更新某个值，过一段时间t之后，所有的副本将达成一致更新成相同的值。暗示，在这个时间t到达之前，各个副本中的数据会出现不一致，而且不一致的方式是不确定的。由于它是微不足道的（仅活泼性），因此没有补充信息就没有用。

说某件事最终最终是一致的，就像说“人们最终死了”。 这是一个非常弱的约束，我们可能希望至少对两件事进行更具体的描述：

- 首先，“最终一致”这里的“最终”会持续多长时间？应该至少了解系统收敛到相同值通常需要多长时间，或者应该有个严格的下限。
- 其次，副本如何在一个价值上达成共识？始终返回“ 42”的系统最终是一致的：所有副本都同意相同的值。它只是不会收敛到有用的值，因为它只会不断返回相同的固定值。相反，我们希望对该方法有一个更好的了解。例如，一种确定的方法是始终获得具有最大时间戳的值。

因此，当供应商说“最终一致性”时，它们的意思是一些更精确的术语，例如“最终最后写入者获胜，并同时读取最新观察到的值”。 “如何写入？” 之所以重要，是因为不良的方法可能导致写入丢失，例如，如果一个节点上的时钟设置不正确并且使用了时间戳。

在有关弱一致性模型的复制方法小节中，我们会更详细地研究这两个问题。

# 3 [Time and order](http://book.mixu.net/distsys/time.html)

# 4 [Replication: preventing divergence](http://book.mixu.net/distsys/replication.html)

# 5 [Replication: accepting divergence](http://book.mixu.net/distsys/eventual.html)

# 6 [Appendix](http://book.mixu.net/distsys/appendix.html)
